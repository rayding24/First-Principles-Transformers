{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "informational-packaging",
   "metadata": {},
   "source": [
    "We will figure out how to properly code out a encoder block along with embeddings, since the transformer variants are made out of stacking these blocks together, part of the reason for the scalability of GPT family is the simplicity of the model design.\n",
    "\n",
    "We use the diagram found in the paper:\n",
    "\n",
    "The first thing we look at is the word embedding. After we use a dictionary to map each word to a unique integer, we would like to learn a more useful embedding. Here, the embedding is just a look up table, with the additional feature that the table can be adjusted by gradient descent. We can dive into algorithms like word2vec in the future if we have time, but for now let's demonstrate why it's really just a look up table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "celtic-surge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "million-johns",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0771,  0.8933, -0.4322,  0.8430,  0.4313],\n",
       "        [-0.9201,  0.5402, -1.1606,  0.2522, -0.9390],\n",
       "        [-1.0256, -0.3078, -2.8334,  2.2777, -0.7678]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = nn.Embedding(3, 5)# vocab size 3, vec dim 5\n",
    "embed.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "patent-advocate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0771,  0.8933, -0.4322,  0.8430,  0.4313],\n",
       "        [-1.0256, -0.3078, -2.8334,  2.2777, -0.7678]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we see that the embedding have a vocabulary of 3 and each vector has length 5\n",
    "# More specifically, we are simply accessing this 2d array with indices when we \n",
    "# call embedding in practice:\n",
    "x = torch.tensor([0,2])\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "removable-reliance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0771,  0.8933, -0.4322,  0.8430,  0.4313],\n",
       "        [-0.9201,  0.5402, -1.1606,  0.2522, -0.9390],\n",
       "        [-0.9201,  0.5402, -1.1606,  0.2522, -0.9390],\n",
       "        [-0.9201,  0.5402, -1.1606,  0.2522, -0.9390],\n",
       "        [-0.9201,  0.5402, -1.1606,  0.2522, -0.9390],\n",
       "        [ 2.0771,  0.8933, -0.4322,  0.8430,  0.4313]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch allows us to access a tensor indefinitely many times with arbitrarily lengthed array\n",
    "x = torch.tensor([0,1,1,1,1,0])\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "checked-things",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d7a6c460a343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# because it is only retriving content using the integer as index, nothing fancy at all\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1850\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# but as soon as we try to embed a bigger integer, it breaks\n",
    "# because it is only retriving content using the integer as index, nothing fancy at all\n",
    "x = torch.tensor([3])\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-karaoke",
   "metadata": {},
   "source": [
    "Now we've seen that embedding is simply a look up table, the next step is positional embedding. Since the word embedding just encodes an integer representing the word in dictionary, it contains no positional information for the incoming words. As we know, in sentences, if we swap orders of words, the meaning can completely change, and we would like the model to learn this semantics, thus we need to device an extra mechanism to bake the positional info into the embeddings which will be fed to the model. \n",
    "\n",
    "The clever thing the authors use is sinusoidal functions.\n",
    "\n",
    "Good explanations can be found at:\n",
    "https://datascience.stackexchange.com/questions/51065/what-is-the-positional-encoding-in-the-transformer-model\n",
    "https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ \n",
    "\n",
    "In essense, let the embedding dimension be D, we slice D sinusoidal curves using postion as the input/independent variable, and use the D outputs as the positional embedding vector. The D sinusoidal functions are at increasing frequencies, which means that even though sinusoidals are periodic functions, the positional embedding vectors are unlikely to overlap (in fact should be garanteed within a range once we view sinusoidals as continuations of binary numbers, but that's for another time). \n",
    "\n",
    "Another really neat feature about this encoding is that it's easy for the model to learn about relative positions, which is really what matters, instead of, say, the absolute position of a word from the start of the article. Also part of the significance is due to the input sequence will be a sliding window through a given text, and the second word in the current input sequence will be the first word in the next, so there's really no point in learning any semantic information based on absolute position. Mathematically, the postional embedding vector as we shift the position by some integer step is a linear transformed (i.e. matrix multiplication) version of the original vector. Intuitively, we can think that the model is really learning these transformation matrices between positions and use that to construct semantics. To wit, we see relative positions, the machine sees transformatioin matrices between the relative positions. The full proof can be found in:\n",
    "\n",
    "https://timodenk.com/blog/linear-relationships-in-the-transformers-positional-encoding/ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "digital-graduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we recycle parts from Pytorch's official positional encoding implementation:\n",
    "# https://pytorch.org/tutorials/beginner/transformer_tutorial.html \n",
    "# which is pretty much\n",
    "# straight up a translation of the formula itself into code so nothing is very new here\n",
    "\n",
    "def positional_encoding(max_len, d_model):\n",
    "    '''\n",
    "    Computes positional embedding vectors deterministically with sin and cos\n",
    "    max_len: number of positions, i.e. input seq. length\n",
    "    d_model: embedding dimensiion\n",
    "    \n",
    "    CAVEAT/WARNING: the embedding dimension must be even, as dictated by the formula\n",
    "    '''\n",
    "    pe = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "#     pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    return pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unavailable-patrol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000],\n",
       "        [ 0.8415,  0.5403,  0.0100,  0.9999],\n",
       "        [ 0.9093, -0.4161,  0.0200,  0.9998],\n",
       "        [ 0.1411, -0.9900,  0.0300,  0.9996],\n",
       "        [-0.7568, -0.6536,  0.0400,  0.9992],\n",
       "        [-0.9589,  0.2837,  0.0500,  0.9988],\n",
       "        [-0.2794,  0.9602,  0.0600,  0.9982],\n",
       "        [ 0.6570,  0.7539,  0.0699,  0.9976],\n",
       "        [ 0.9894, -0.1455,  0.0799,  0.9968],\n",
       "        [ 0.4121, -0.9111,  0.0899,  0.9960]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the positional vectors for input sequence length of 10 and embedding dim 5\n",
    "# As a quick sanity check, notice the first row should be\n",
    "# [sin(0), cos(0), sin(0), cos(0)] = [0,1,0,1] checks out!\n",
    "positional_encoding(10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "loving-archives",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 5.2250e-01,  7.4871e-01, -6.9554e-01, -4.6671e-01],\n",
      "        [ 5.9929e-01, -1.7954e+00,  8.9031e-01,  9.3008e-01],\n",
      "        [-4.3043e-01,  7.2504e-01,  6.1766e-01,  8.5517e-01],\n",
      "        [ 1.1575e+00, -5.3880e-01,  8.1583e-01, -5.1187e-01],\n",
      "        [-1.6941e+00, -4.1849e-01, -7.3580e-01,  9.9257e-02],\n",
      "        [-4.4127e-01,  8.5713e-02,  1.3710e+00,  2.8298e-01],\n",
      "        [ 1.3788e+00, -2.5741e-04, -7.7389e-01,  3.4132e+00],\n",
      "        [ 5.2250e-01,  7.4871e-01, -6.9554e-01, -4.6671e-01],\n",
      "        [ 2.7098e+00,  1.3977e+00,  9.7662e-02,  8.0236e-01],\n",
      "        [-5.9547e-01,  6.5157e-01,  5.8158e-01,  8.8459e-01]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "tensor([[ 0.5225,  1.7487, -0.6955,  0.5333],\n",
      "        [ 1.4408, -1.2551,  0.9003,  1.9300],\n",
      "        [ 0.4789,  0.3089,  0.6377,  1.8550],\n",
      "        [ 1.2987, -1.5288,  0.8458,  0.4877],\n",
      "        [-2.4509, -1.0721, -0.6958,  1.0985],\n",
      "        [-1.4002,  0.3694,  1.4210,  1.2817],\n",
      "        [ 1.0994,  0.9599, -0.7139,  4.4114],\n",
      "        [ 1.1795,  1.5026, -0.6256,  0.5308],\n",
      "        [ 3.6991,  1.2522,  0.1776,  1.7992],\n",
      "        [-0.1834, -0.2596,  0.6715,  1.8805]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Next, all we need to do is to add to the word embedding\n",
    "embed = nn.Embedding(20, 4) # vocab 20, vector dim 4\n",
    "x = torch.tensor([0,1,7,3,8,5,4,0, 17, 12])\n",
    "x = embed(x)\n",
    "pe = positional_encoding(10, 4)\n",
    "print(x)\n",
    "print(x+pe)\n",
    "x = x+pe\n",
    "# We can visually check it is simply the elementwise addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-smell",
   "metadata": {},
   "source": [
    "Now we are ready to move on to the encoder block, which as shown in the diagram, consists of self-attention, layer norm and feed forward network. \n",
    "\n",
    "We have covered attention, feed forward network is simply linear transformation (matrix multiplication and bias) combined with a non-linear activation function (e.g. ReLU). \n",
    "\n",
    "Layer norm (https://arxiv.org/pdf/1607.06450.pdf) is essentially re-centering the embedding distribution to 0 with standard deviation of 1, except it has some additional learned parameters to massage the distribution further to according to the data, but this does not change the gist.\n",
    "\n",
    "Let's do a step-through for the encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "emerging-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've previously hacked out attention as:\n",
    "def _self_attention(x, emb_dim, latent_dim):\n",
    "    M_K, M_Q, M_V = [torch.rand(emb_dim, latent_dim) for _ in range(3)]\n",
    "    K, Q, V = x@M_K, x@M_Q, x@M_V \n",
    "    W_raw = Q@(K.transpose(1,2))\n",
    "    W = F.softmax(W_raw, dim=1)\n",
    "    Y = W@V\n",
    "    return Y\n",
    "\n",
    "# while it illustrates the key concepts, this is not most efficient/standard way to implement \n",
    "# In practice, we could use the Pytorch linear layer to do the matrix work for us\n",
    "# And we actually want the latent dim to be the same as embedding dim just to make things\n",
    "# simpler and easier to contruct residual connections\n",
    "# We also negative indices to avoid having to deal with the batch dimension which is usually the first\n",
    "# in this case we are not batching yet and this code will not be affected whereas positive indices will be\n",
    "\n",
    "def self_attention(x, emb_dim):\n",
    "    M_K, M_Q, M_V = [nn.Linear(emb_dim, emb_dim, bias=False) for _ in range(3)]\n",
    "    K, Q, V = [M(x) for M in [M_K, M_Q, M_V ]]\n",
    "    W_raw = Q@(K.transpose(-1,-2))\n",
    "    W = F.softmax(W_raw, dim=-1)\n",
    "    Y = W@V\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "extended-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0327,  0.5757,  0.1935,  0.1563],\n",
      "        [-0.0577,  0.2456,  0.0520,  0.0041],\n",
      "        [ 0.0533,  0.4262,  0.1914,  0.2409],\n",
      "        [-0.2851,  0.3682, -0.0097, -0.2671],\n",
      "        [-0.1351,  0.8174,  0.2423,  0.1249],\n",
      "        [ 0.0053,  1.4199,  0.5294,  0.4709],\n",
      "        [ 0.6515,  0.2615,  0.4021,  0.9457],\n",
      "        [-0.0649,  0.4225,  0.1173,  0.0577],\n",
      "        [ 0.0294,  0.1864,  0.0610,  0.0835],\n",
      "        [ 0.0262,  0.4751,  0.2000,  0.2282]], grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_attn = self_attention(x, 4)\n",
    "print(x_attn)\n",
    "x = x+x_attn # residual/addition connection'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "missing-serum",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3288,  1.5076, -1.2692,  0.0904],\n",
      "        [ 0.4799, -1.4269, -0.3281,  1.2751],\n",
      "        [-0.7354, -0.3693, -0.6123,  1.7169],\n",
      "        [ 0.9439, -1.6826,  0.3181,  0.4205],\n",
      "        [-1.2941,  0.4488, -0.5171,  1.3625],\n",
      "        [-1.7049,  0.7377,  0.2817,  0.6854],\n",
      "        [ 0.2013, -0.4740, -1.2234,  1.4961],\n",
      "        [ 0.3179,  1.2512, -1.5270, -0.0421],\n",
      "        [ 1.3514, -0.4557, -1.3324,  0.4367],\n",
      "        [-1.0831, -0.3047, -0.2470,  1.6349]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "tensor([[-0.3271, -0.1151,  0.3035, -0.1041],\n",
      "        [-0.1208, -0.1754,  0.1748, -0.2144],\n",
      "        [-0.0746, -0.2061,  0.1998, -0.1201],\n",
      "        [-0.1316, -0.2053,  0.1774, -0.2689],\n",
      "        [-0.0535, -0.2428,  0.1933, -0.0846],\n",
      "        [-0.0279, -0.3096,  0.1677, -0.0493],\n",
      "        [-0.1461, -0.1492,  0.1999, -0.1966],\n",
      "        [-0.4272, -0.0251,  0.3488, -0.1124],\n",
      "        [-0.3756, -0.0784,  0.2905, -0.1758],\n",
      "        [-0.0549, -0.2382,  0.1955, -0.0863]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.6559,  1.3925, -0.9658, -0.0137],\n",
      "        [ 0.3591, -1.6023, -0.1533,  1.0607],\n",
      "        [-0.8100, -0.5754, -0.4124,  1.5969],\n",
      "        [ 0.8123, -1.8879,  0.4955,  0.1516],\n",
      "        [-1.3476,  0.2060, -0.3238,  1.2778],\n",
      "        [-1.7328,  0.4281,  0.4494,  0.6361],\n",
      "        [ 0.0551, -0.6232, -1.0235,  1.2995],\n",
      "        [-0.1093,  1.2260, -1.1782, -0.1545],\n",
      "        [ 0.9758, -0.5342, -1.0419,  0.2608],\n",
      "        [-1.1380, -0.5430, -0.0516,  1.5486]], grad_fn=<AddBackward0>)\n",
      "tensor([[-0.6565,  1.6030, -0.9983,  0.0519],\n",
      "        [ 0.4536, -1.5544, -0.0710,  1.1718],\n",
      "        [-0.7902, -0.5463, -0.3767,  1.7132],\n",
      "        [ 0.8720, -1.6890,  0.5716,  0.2454],\n",
      "        [-1.3735,  0.2670, -0.2924,  1.3988],\n",
      "        [-1.7260,  0.4967,  0.5187,  0.7107],\n",
      "        [ 0.1454, -0.6244, -1.0786,  1.5575],\n",
      "        [-0.0648,  1.4993, -1.3168, -0.1178],\n",
      "        [ 1.3802, -0.5847, -1.2453,  0.4498],\n",
      "        [-1.0944, -0.4981, -0.0056,  1.5981]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "# construct LayerNorm layers\n",
    "ln1 = nn.LayerNorm(emb_dim)\n",
    "ln2 = nn.LayerNorm(emb_dim)\n",
    "\n",
    "x_ln1 = ln1(x)\n",
    "print(x_ln1)\n",
    "\n",
    "# then we let each word/vector flow through the same feed forward network/multi-layer perceptron(MLP)\n",
    "# which in pytorch is simply\n",
    "\n",
    "mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2*emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*emb_dim, emb_dim) ) # can be anything mlp, this is one simple example\n",
    "\n",
    "x_mlp = mlp(x)\n",
    "print(x_mlp)\n",
    "\n",
    "#residual connection again\n",
    "x = x_ln1 + x_mlp\n",
    "print(x)\n",
    "\n",
    "#final layer norm\n",
    "x = ln2(x)\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting all the steps together, we can organize the encoder block into a class like this:\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2*emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*emb_dim, emb_dim) ) # can be any mlp, this is one simple example\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "\n",
    "    def self_attention(self, x, emb_dim,):\n",
    "        M_K, M_Q, M_V = [nn.Linear(emb_dim, emb_dim, bias=False) for _ in range(3)]\n",
    "        K, Q, V = [M(x) for M in [M_K, M_Q, M_V ]]\n",
    "        W_raw = Q@(K.transpose(-1,-2))\n",
    "        # == masking begins ==\n",
    "        ones = torch.ones((seq_len, seq_len), dtype=torch.uint8)\n",
    "        mask = torch.triu(ones, diagonal=1)\n",
    "        W_raw[mask] = float('-inf')\n",
    "        # == masking ends ==\n",
    "        W = F.softmax(W_raw, dim=-1)\n",
    "        Y = W@V\n",
    "        return Y\n",
    "\n",
    "\n",
    "\n",
    "    def forward(x):\n",
    "        x = x + self.self_attention(x, self.emb_dim)\n",
    "        x_ln1 = self.ln1(x)\n",
    "        x_mlp = self.mlp(x)\n",
    "        x = x_ln1 + x_mlp\n",
    "        x = ln2(x)\n",
    "        return x \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alone-compiler",
   "metadata": {},
   "source": [
    "That's it! We've run through an encoder block. In reality, we would use a bigger embedding dimension, more complex MLP, batching, etc. But the essence of the transformer has been captured here and will not change in any major way. \n",
    "\n",
    "We will visit the last key idea, masking, in the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-burton",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
