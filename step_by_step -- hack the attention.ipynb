{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SO', 'to', 'begin', 'with', 'I', 'have', 'a', 'sentence']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SO to begin with I have a sentence\n",
    "sentence = 'SO to begin with I have a sentence'.split(' ')\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SO': 0,\n",
       " 'to': 1,\n",
       " 'begin': 2,\n",
       " 'with': 3,\n",
       " 'I': 4,\n",
       " 'have': 5,\n",
       " 'a': 6,\n",
       " 'sentence': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to convert sentence to numbers before converting to tensor\n",
    "word2idx = {}\n",
    "count = 0\n",
    "for word in sentence:\n",
    "    if word in word2idx:\n",
    "        continue\n",
    "    else:\n",
    "        word2idx[word] = count\n",
    "        count += 1\n",
    "    \n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "tensor([[[-0.7012,  0.9257,  0.3939,  0.7522,  0.5292],\n",
      "         [-0.2357, -1.1488, -0.0339,  0.9371, -0.8041],\n",
      "         [-1.1951,  1.1564, -0.5757, -0.5994, -0.8532],\n",
      "         [-0.2493, -0.8708, -2.2939,  1.1188,  0.9900],\n",
      "         [-0.2316, -1.3752, -1.1277,  1.1448,  1.4320],\n",
      "         [ 1.5041, -0.5158,  1.9260, -0.8586, -1.0676],\n",
      "         [ 0.4384, -0.1313, -1.1147,  0.2073, -0.2337],\n",
      "         [-0.4124, -0.6262,  0.0381,  0.3523, -0.9138]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# then there was some kind of embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[word2idx[word] for word in sentence]]) # batch 1 for sentence\n",
    "print(x)\n",
    "embed = nn.Embedding(20, 5) #vocab 20 and vec size 5\n",
    "x = embed(x)\n",
    "print(x)\n",
    "# so each word -> number -> vector, sure, that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there was this positional encoding thing, but let's skip it since it's added, ie assume 0\n",
    "# then we got multiheaded attention\n",
    "# So attention can be seen as this heuristic:\n",
    "# IN ESSENCE -- add more changeable parameters/degree of freedom to the most BASIC approach\n",
    "# 1. We want seq -> seq, and somehow the output seq captures the correlations with input seq\n",
    "# 2. A natural way to do this is just weighted sum\n",
    "# 3. A natural way for weights is just dot products\n",
    "# 4. To make things easier, we normalize, with softmax\n",
    "# 5. The raw vectors may not be in the right latent space, let's add an extra linear transf.\n",
    "# 6. AND, let each 'raw vec term' in the weighted sum have its own customizable projection\n",
    "# 7. Voila! These transformed raw vectors are key, quary and values! \n",
    "#    And we get the output seq by good old weighted sum (weights as dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 5\n",
    "latent_dim = 7\n",
    "#init the matrices to get K, Q, V vecotors\n",
    "M_K, M_Q, M_V = [torch.rand(emb_dim, latent_dim) for _ in range(3)]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4851, 0.9903, 0.7989, 0.6851, 0.6765, 0.3137, 0.6337],\n",
       "        [0.3811, 0.2715, 0.2483, 0.5626, 0.2533, 0.5040, 0.2638],\n",
       "        [0.3329, 0.1015, 0.6938, 0.3812, 0.1823, 0.2009, 0.5130],\n",
       "        [0.3601, 0.2882, 0.0179, 0.9687, 0.8276, 0.3333, 0.3122],\n",
       "        [0.2598, 0.6326, 0.2150, 0.5138, 0.3100, 0.7412, 0.4021]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5521,  0.1484,  0.0702,  1.1911,  0.6184,  0.9685,  0.4495],\n",
       "         [-0.4349, -0.7874, -0.6532, -0.3261,  0.0697, -0.9434, -0.5006],\n",
       "         [-0.7682, -1.6405, -1.2613, -1.4068, -1.3810, -0.7399, -1.2778],\n",
       "         [-0.5563,  0.2326, -1.7740,  0.0575,  0.4254,  0.1287, -0.8170],\n",
       "         [-0.2276,  0.5186, -0.9805,  0.4826,  0.6806,  0.4505, -0.1548],\n",
       "         [ 0.5877,  0.6222,  2.1649,  0.0942,  0.1966, -0.4785,  1.1077],\n",
       "         [-0.1945,  0.1972, -0.5023, -0.1176,  0.1592, -0.2567, -0.3579],\n",
       "         [-0.5367, -1.0512, -0.6488, -0.7487, -0.4224, -0.9973, -0.6645]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@M_K #this is how one transforms the whole batch of words!\n",
    "# the trick to applying matrix to a batch is really just put x first \n",
    "# it can be hard to make that mental switch from the math background\n",
    "# since in math we are so used to put the matrix before x, but nothing special is going on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "K, Q, V = x@M_K, x@M_Q, x@M_V \n",
    "# to get all pairs of dot products between Q_i and K_j, we multiply the \"batch matrices\" like so\n",
    "# here matrices is truly a bookkeeping device\n",
    "# and the matrix multiplication here is just a neat notation to get all pairs\n",
    "# almost like list comprehension is a neat notation for for loop, the essence remains basic\n",
    "# W_ij = Q_i . K_j\n",
    "W_raw = Q@(K.transpose(1,2))\n",
    "# when we do the final sum, we are summing over (or contracting the index of;) j -- the keys\n",
    "# and we softmax these weights, i.e softmax all the rows of W\n",
    "W = F.softmax(W_raw, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check softmax does indeed makes probabilities\n",
    "torch.sum(W, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9371,  0.8236,  1.2489,  1.8378,  1.4066,  0.6687,  0.4845],\n",
       "         [-0.0982, -0.1767, -0.2936, -0.8520, -0.4210, -0.2800, -0.3303],\n",
       "         [-0.3392, -0.8299, -1.0265, -1.5608, -1.9907, -1.6549, -0.9218],\n",
       "         [-0.2412, -0.2690, -0.4956, -1.7746, -0.5718, -0.1423, -0.5431],\n",
       "         [-0.0947, -0.0421, -0.0998, -1.0533,  0.0788,  0.4430, -0.2389],\n",
       "         [ 0.1258,  0.1192,  0.1740,  0.0719,  0.2455,  0.2015,  0.0255],\n",
       "         [-0.0516, -0.0684, -0.1397, -0.5674, -0.1472, -0.0428, -0.1848],\n",
       "         [-0.1221, -0.3061, -0.4342, -1.0544, -0.7245, -0.5557, -0.4898]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting it together: y_i = \\sum_j{ W_i_j * v_j}\n",
    "# in matrix form -- one could check by hand, it is\n",
    "Y = W@V\n",
    "# so yeah, batching can use matrix operations to bookkeep things and results elegantly!\n",
    "Y\n",
    "# num_words x latent dim -- shape checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiheaded attention (wide) is just repeating the process a few times to get different Y's\n",
    "# this way we can have multiple customizable \"contexts\" in these Y's\n",
    "# the hope is that backprop will tune it such that each Y represents a disentabgled context\n",
    "# Well then, let's make attention a function and call it repeatedly\n",
    "def self_attention(emb_dim, latent_dim):\n",
    "    M_K, M_Q, M_V = [torch.rand(emb_dim, latent_dim) for _ in range(3)]\n",
    "    K, Q, V = x@M_K, x@M_Q, x@M_V \n",
    "    W_raw = Q@(K.transpose(1,2))\n",
    "    W = F.softmax(W_raw, dim=1)\n",
    "    Y = W@V\n",
    "    return Y\n",
    "num_heads = 9\n",
    "Ys = [self_attention(5,7) for _ in range(num_heads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 0.1381,  0.1175,  0.2261,  0.0956,  0.1979,  0.1973,  0.1280],\n",
       "          [ 0.0314, -0.6409,  0.1912, -0.3636, -0.8466,  0.0484, -1.0450],\n",
       "          [-1.7420, -2.7032, -1.9559, -2.8386, -5.1617, -2.1685, -4.6580],\n",
       "          [-0.0170, -0.2379,  0.0393, -0.1323, -0.3201,  0.0066, -0.3718],\n",
       "          [ 0.0767, -0.0820,  0.1082, -0.0438, -0.0887,  0.0348, -0.1689],\n",
       "          [ 0.4044,  1.1314,  0.9159,  1.5698,  2.3812,  1.7393,  2.4376],\n",
       "          [ 0.0523, -0.0546,  0.0709, -0.0306, -0.0591,  0.0208, -0.1136],\n",
       "          [-0.1072, -0.8400,  0.0406, -0.5487, -1.2029, -0.0816, -1.3557]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 3.5052e-01,  4.1408e-01,  4.7629e-01,  5.0054e-01,  2.1207e-01,\n",
       "            3.7885e-01,  6.6828e-01],\n",
       "          [ 2.6067e-02,  3.5265e-02, -3.4085e-01,  3.1393e-01,  5.8668e-02,\n",
       "            1.6751e-01, -1.6683e-02],\n",
       "          [-4.1660e-01, -2.4759e-01, -6.4185e-01, -3.2129e-01, -3.3837e-01,\n",
       "           -1.3594e-01, -6.0418e-01],\n",
       "          [-2.4370e+00, -1.8230e+00, -3.8138e+00, -1.2455e+00, -2.2855e+00,\n",
       "           -9.2446e-01, -3.2197e+00],\n",
       "          [-3.6784e-02, -5.2299e-03, -3.7547e-01,  2.1919e-01, -7.5534e-04,\n",
       "            1.2124e-01, -9.6310e-02],\n",
       "          [ 4.9927e-01,  6.9110e-01,  1.6271e+00,  2.3895e-01,  1.1416e+00,\n",
       "            6.6450e-01,  2.3102e-01],\n",
       "          [-2.9443e-02, -5.5914e-03, -1.5324e-01,  6.4935e-02, -9.2923e-04,\n",
       "            4.6580e-02, -6.8112e-02],\n",
       "          [-1.3297e-02,  1.7923e-02, -2.7321e-01,  1.6108e-01,  1.9177e-02,\n",
       "            1.0302e-01, -6.1530e-02]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 0.7802,  0.2536,  1.2379,  1.7166,  1.4808,  1.3835,  1.3613],\n",
       "          [-0.3849, -0.1108, -0.7565, -1.1387, -1.1188, -0.8935, -0.6905],\n",
       "          [-0.7210, -0.2991, -1.5144, -2.8299, -1.9650, -1.0708, -1.1708],\n",
       "          [-0.1759, -0.0239, -0.3261, -0.3693, -0.4904, -0.4827, -0.3324],\n",
       "          [ 0.0756, -0.0192,  0.1259, -0.0252,  0.1249,  0.0835,  0.1051],\n",
       "          [ 0.0436, -0.0418,  0.0703, -0.1592,  0.1799,  0.2838,  0.1073],\n",
       "          [-0.0718, -0.0225, -0.1609, -0.3282, -0.1538, -0.0743, -0.1165],\n",
       "          [-0.5589, -0.2030, -1.1269, -1.8842, -1.6888, -1.2492, -0.9922]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 0.8162,  1.7343,  0.1056,  1.9052,  0.4980,  1.1376,  2.1401],\n",
       "          [ 0.2140, -1.8369,  0.1462, -1.0119, -0.1404, -0.7871, -2.0140],\n",
       "          [-0.1207, -1.0545, -0.1724, -0.5797, -0.0979, -0.4970, -1.1173],\n",
       "          [-1.2341, -1.0563, -0.8184, -1.1525, -0.1692, -0.6792, -1.2942],\n",
       "          [-0.0510, -0.0854, -0.0443, -0.0596, -0.0107, -0.0485, -0.0923],\n",
       "          [ 0.1056,  0.1197,  0.0795,  0.1493,  0.0851,  0.0987,  0.1107],\n",
       "          [-0.0297, -0.1833, -0.0427, -0.0887, -0.0165, -0.0938, -0.1891],\n",
       "          [ 0.0563, -1.7033,  0.0090, -0.9651, -0.1401, -0.7536, -1.8607]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 1.4428e+00,  1.6804e+00,  8.4334e-01,  1.1159e+00,  1.7440e+00,\n",
       "            1.1494e+00,  7.3315e-01],\n",
       "          [-3.6711e-01, -2.8009e-01, -2.7054e-01, -2.1521e-01, -2.8099e-01,\n",
       "           -8.3601e-02, -1.1662e-01],\n",
       "          [-5.4280e+00, -3.8571e+00, -2.5850e+00, -3.7081e+00, -4.1608e+00,\n",
       "           -2.9551e+00, -2.0861e+00],\n",
       "          [-2.2426e-01, -1.9432e-01, -3.7006e-01, -1.0184e-01, -1.4235e-01,\n",
       "            1.6278e-01, -1.0079e-01],\n",
       "          [-2.3817e-02, -3.6736e-02, -2.0415e-01, -1.0421e-02,  1.5088e-02,\n",
       "            1.7581e-01, -4.9192e-02],\n",
       "          [ 4.4911e-01,  5.2420e-01,  2.7538e-01,  3.0708e-01,  5.4691e-01,\n",
       "            3.1954e-01,  2.0738e-01],\n",
       "          [-1.1040e-03,  1.5750e-02, -6.2481e-02, -5.1299e-02,  4.2892e-02,\n",
       "            4.2927e-02, -3.8593e-02],\n",
       "          [-8.6727e-01, -6.4907e-01, -4.7144e-01, -5.4968e-01, -6.9119e-01,\n",
       "           -3.9555e-01, -2.8744e-01]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 1.9708e-03, -1.4184e-01,  1.4804e-01, -4.8453e-02,  3.3531e-01,\n",
       "            5.1110e-02,  3.9712e-01],\n",
       "          [-8.5544e-02, -7.9926e-02,  1.1277e-02, -7.0281e-02,  5.8830e-02,\n",
       "           -4.3634e-02,  6.6390e-02],\n",
       "          [-3.1197e+00, -4.2470e+00, -2.3254e+00, -3.0796e+00, -5.0223e-01,\n",
       "           -1.7578e+00, -9.0691e-01],\n",
       "          [-1.8919e+00, -6.8627e-01, -1.6177e+00, -1.5681e+00, -1.4365e+00,\n",
       "           -1.4219e+00, -1.2130e+00],\n",
       "          [ 5.5798e-01,  1.2872e+00,  4.8652e-01,  6.6939e-01, -2.4574e-01,\n",
       "            1.2048e-01, -3.0542e-01],\n",
       "          [ 8.2337e-01, -4.9083e-02,  4.8374e-01,  2.7115e-01,  1.1356e+00,\n",
       "            5.6925e-01,  1.3393e+00],\n",
       "          [-8.4996e-02, -7.4034e-02, -7.2038e-03, -6.9955e-02,  3.3303e-02,\n",
       "           -4.5793e-02,  4.0831e-02],\n",
       "          [-2.0615e-01, -2.5971e-01, -7.1092e-02, -1.9372e-01,  6.4622e-02,\n",
       "           -1.0467e-01,  6.4538e-02]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 2.0838,  0.3306,  1.3016,  2.1941,  0.5785,  0.2456,  0.8580],\n",
       "          [-0.6190, -0.2528, -0.9330,  0.1695, -0.5912, -0.9633, -0.1440],\n",
       "          [-1.1753, -0.4598, -1.1048, -0.7883, -0.7398, -0.7801, -0.3709],\n",
       "          [-1.2816, -0.9411, -1.0375, -1.9038, -0.7244, -0.5408, -0.3790],\n",
       "          [-0.0525, -0.0851, -0.0770, -0.1069, -0.0437, -0.0819,  0.0221],\n",
       "          [ 0.3598, -0.2606, -0.3187,  0.7347, -0.2099, -0.7011,  0.0996],\n",
       "          [-0.3121, -0.2191, -0.2756, -0.4114, -0.1815, -0.2029, -0.0113],\n",
       "          [-0.8211, -0.2637, -1.0010, -0.0644, -0.6427, -0.9065, -0.2423]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 0.4924,  0.8361,  0.5205,  0.2576,  0.2035,  0.7298,  1.0483],\n",
       "          [-0.2728, -0.8854, -0.4583, -0.3874, -0.4456, -0.3407, -0.5378],\n",
       "          [-0.6827, -1.1559, -0.6103, -0.7106, -0.6675, -0.5471, -1.5066],\n",
       "          [-0.0345, -0.7488, -0.0073, -0.1707, -0.2945, -0.2077, -0.6266],\n",
       "          [ 0.1333, -0.3373, -0.0020,  0.1373,  0.0654, -0.1907,  0.0868],\n",
       "          [ 1.0696,  0.3806,  0.7674,  0.8220,  0.5015,  0.4621,  1.6421],\n",
       "          [ 0.0322, -0.1761,  0.0274,  0.0251, -0.0067, -0.0790, -0.0974],\n",
       "          [-0.3924, -0.9267, -0.5812, -0.4901, -0.5124, -0.3841, -0.6518]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 2.0377e+00,  1.1982e+00,  7.5057e-01,  1.5677e+00,  2.0172e+00,\n",
       "            8.1964e-01,  1.8022e+00],\n",
       "          [-1.0342e-01, -1.1002e-01, -3.5409e-02, -1.7443e-02, -1.9251e-01,\n",
       "            4.6234e-02, -9.0147e-02],\n",
       "          [-4.9245e+00, -3.3666e+00, -1.7692e+00, -2.1972e+00, -4.5467e+00,\n",
       "           -2.1524e-02, -4.2289e+00],\n",
       "          [ 1.0226e-02, -3.0587e-02, -5.4556e-04,  3.0967e-02, -9.4909e-02,\n",
       "            4.5390e-02,  1.3736e-02],\n",
       "          [ 2.0888e-01,  9.7563e-02,  1.0665e-01,  1.4164e-01,  1.2082e-01,\n",
       "            1.0133e-01,  1.7143e-01],\n",
       "          [-3.3514e-02, -5.0398e-02, -2.0250e-03,  3.8731e-03, -7.6085e-02,\n",
       "            1.7568e-02, -4.5923e-02],\n",
       "          [-6.6900e-02, -7.6272e-02, -2.3392e-02, -1.1935e-02, -1.4487e-01,\n",
       "            3.3038e-02, -5.7227e-02],\n",
       "          [-5.0745e-01, -4.0065e-01, -1.9282e-01, -1.6428e-01, -5.8675e-01,\n",
       "            7.8120e-02, -4.2871e-01]]], grad_fn=<UnsafeViewBackward>)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ys\n",
    "# well, this is definitely not the best way to do it for pytorch since this could cause \n",
    "# gradient flowing issues, nonetheless, we do it here for understanding the essence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.8949,  0.9564,  0.0245,  0.6524,  0.2615],\n",
       "         [ 0.0298, -1.2436,  0.3113,  1.6654, -0.7629],\n",
       "         [-0.9579,  1.9236, -0.1989, -0.2279, -0.5390],\n",
       "         [ 0.0093, -0.4829, -1.6098,  1.0927,  0.9907],\n",
       "         [-0.1745, -1.1719, -0.9561,  1.0260,  1.2765],\n",
       "         [ 1.0381, -0.5669,  1.3733, -0.8392, -1.0053],\n",
       "         [ 1.1394,  0.0668, -1.7847,  0.7044, -0.1260],\n",
       "         [-0.2198, -0.6898,  0.7704,  1.4611, -1.3219]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, two common tricks for neural net training:\n",
    "# 1. Residual connection:\n",
    "# if we want to add a residual connection, i.e. add Xs to Ys, we need to make them \n",
    "# the same dimension, i.e. latent dim = emb dim\n",
    "# residual connection helps the gradient flow through deep neural nets, preventing the \n",
    "# gradient update from getting stuck \n",
    "# 2. Normalization: standardize data, stablize and accelerate training\n",
    "# in this case we use layer norm on embedding dimension\n",
    "# We don't want to worry about layer norm here since that's not essnetial to the \n",
    "# transfromer architechture and can be changed for different contexts (e.g. vision)\n",
    "m = nn.LayerNorm(emb_dim)\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7012,  0.9257,  0.3939,  0.7522,  0.5292],\n",
       "         [-0.2357, -1.1488, -0.0339,  0.9371, -0.8041],\n",
       "         [-1.1951,  1.1564, -0.5757, -0.5994, -0.8532],\n",
       "         [-0.2493, -0.8708, -2.2939,  1.1188,  0.9900],\n",
       "         [-0.2316, -1.3752, -1.1277,  1.1448,  1.4320],\n",
       "         [ 1.5041, -0.5158,  1.9260, -0.8586, -1.0676],\n",
       "         [ 0.4384, -0.1313, -1.1147,  0.2073, -0.2337],\n",
       "         [-0.4124, -0.6262,  0.0381,  0.3523, -0.9138]]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
