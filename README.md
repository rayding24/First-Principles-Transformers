# Transformers implemented and explained from scratch, culminating in (smaller scale) GPT3 and Vision Transformer (ViT)

First Principles Transformers-Vanilla2SOTA


# Biblio:

## Original papers:

## Folders:
_minGPT and _fullGPT are adapted from, respectively, karpathy/minGPT and  tingkai-zhang/openai-gpt-pytorch (which is modified from HuggingFace's gpt models) for the purpose of learning and understanding transformers at scale, since these are among the best implementations the author can find.

## The following material has been very inspirational and helpful for understanding transformers better:
http://peterbloem.nl/blog/transformers
http://jalammar.github.io/illustrated-transformer/
http://juditacs.github.io/2018/12/27/masked-attention.html 

## These material has been interesting to explore:
https://nlp.seas.harvard.edu/2018/04/03/attention.html 
https://kazemnejad.com/blog/transformer_architecture_positional_encoding/ 
https://stackoverflow.com/questions/50747947/embedding-in-pytorch 
https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/ 
