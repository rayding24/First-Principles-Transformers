# Transformers implemented and explained from scratch, culminating in (smaller scale) GPT3 and Vision Transformer (ViT)

First Principles Transformers-Vanilla2SOTA


# Biblio:
The author wish to express sincere gratitude towards the following sources, without which this journey would have taken much longer and been more difficult.

## Original papers:

## Folders:
_minGPT and _fullGPT are mostly adapted from, respectively, karpathy/minGPT and  tingkai-zhang/openai-gpt-pytorch (which is modified from HuggingFace's gpt models). The author reimplemented them about halfway through the project for the purpose of learning and understanding transformers at scale, since these are among the best implementations the author can find, certaintly more native and efficient than the GPT the author would have wrote without the learning process.

## The following material has been very inspirational and helpful for understanding transformers better:
http://peterbloem.nl/blog/transformers  
http://jalammar.github.io/illustrated-transformer/  
http://juditacs.github.io/2018/12/27/masked-attention.html   

## These material has been interesting to explore:
https://nlp.seas.harvard.edu/2018/04/03/attention.html   
https://kazemnejad.com/blog/transformer_architecture_positional_encoding/   
https://stackoverflow.com/questions/50747947/embedding-in-pytorch   
https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/   
