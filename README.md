# Transformers implemented and explained from scratch, culminating in (smaller scale) GPT3 and iGPT

An end-to-end walk through of transformer architecture and heuristics, for my own (and potentially others') learning purposes. Updated continuously. 

### Current Version:

- [x] Self-Attention
- [x] Block Architecture
- [x] Word Embedding
- [x] Positional Embedding
- [x] Word Embedding
- [x] Masking 
- [x] Transformer Architecture
- [x] GPT Architecture

### TODO:

- [ ] Training
- [ ] GPT training
- [ ] iGPT training

### Optional Exploration:

- [ ] Newer Attention Mechanisms with better time complexity (Linformer, Reformer, etc.)
- [ ] Computer Vision (Vision Transformer)
- [ ] PyTorch Lightning refactor


# Biblio:
The author wish to acknowledge the following sources (in-text citations can also be found in the notebooks).

## Original papers:
[Attention Is All You Need](https://arxiv.org/abs/1706.03762) \
[Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\
[Layer Normalization](https://arxiv.org/abs/1607.06450) \
[Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\



## Folder:
_minGPT is adapted from  \
karpathy/minGPT:https://github.com/karpathy/minGPT/blob/master  


## The following material has been very inspirational and helpful for understanding transformers better:
http://peterbloem.nl/blog/transformers  \
http://jalammar.github.io/illustrated-transformer/  \
https://jalammar.github.io/illustrated-gpt2/ \
http://juditacs.github.io/2018/12/27/masked-attention.html   \

## These material has been interesting to explore:
https://nlp.seas.harvard.edu/2018/04/03/attention.html   \
https://kazemnejad.com/blog/transformer_architecture_positional_encoding/   \
https://stackoverflow.com/questions/50747947/embedding-in-pytorch   \
https://www.reddit.com/r/MachineLearning/comments/cttefo/d_positional_encoding_in_transformer/   \

