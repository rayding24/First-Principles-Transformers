{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "import torch.nn.functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get real! Now we know attention, which is of course \"all you need\" ;) We will \n",
    "# implement encoder properly with in pytorch\n",
    "# we use einops to simplify matrix handling syntax among other things, to install, uncomment:\n",
    "# !pip3 install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__) # einsum in early versions can be too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.1204e-01, 3.3179e-01, 3.7844e-01, 7.2353e-01, 4.4927e-01, 1.1372e-01,\n",
       "         2.7527e-01, 2.0844e-01, 3.0714e-01, 1.5609e-01],\n",
       "        [4.5294e-01, 9.9741e-01, 3.5441e-01, 2.1147e-01, 1.8738e-01, 7.6277e-01,\n",
       "         5.6122e-01, 9.7209e-04, 2.1788e-01, 8.6360e-01],\n",
       "        [3.4282e-01, 6.3288e-01, 5.9995e-01, 5.8601e-01, 7.1222e-02, 5.2544e-01,\n",
       "         3.2955e-01, 2.8433e-01, 3.7653e-01, 6.3157e-01],\n",
       "        [8.2733e-01, 4.8969e-01, 4.8225e-01, 1.3652e-02, 5.5434e-01, 2.6493e-01,\n",
       "         2.8982e-01, 6.8646e-01, 1.6388e-01, 4.6974e-01],\n",
       "        [5.5633e-01, 5.6522e-01, 9.8118e-01, 4.0900e-01, 2.4492e-01, 1.8387e-01,\n",
       "         2.1073e-01, 7.3774e-01, 1.6522e-01, 6.4002e-01],\n",
       "        [5.3758e-01, 8.9740e-01, 8.6875e-01, 8.8043e-01, 1.6907e-01, 3.2457e-01,\n",
       "         7.3095e-01, 7.8696e-01, 7.5072e-01, 4.4058e-01],\n",
       "        [9.2194e-01, 4.5239e-01, 8.8996e-01, 9.5824e-01, 6.2432e-01, 6.7026e-01,\n",
       "         7.6904e-01, 5.4834e-01, 5.6674e-01, 9.1058e-01],\n",
       "        [8.5816e-01, 5.0199e-01, 5.0019e-01, 9.2626e-01, 3.1948e-01, 9.6178e-01,\n",
       "         9.2905e-01, 7.1913e-01, 2.6952e-01, 8.6636e-02],\n",
       "        [4.7680e-01, 5.1278e-01, 1.4902e-01, 2.2101e-02, 1.5706e-01, 8.4604e-01,\n",
       "         2.7758e-01, 7.8745e-01, 8.0730e-01, 9.2942e-01]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "BLOCK:\n",
    "batch of tensors of embedding dim n -> self attention + id\n",
    "-> layer norm -> MLP for each in batch (just plain default) + id\n",
    "-> layer norm\n",
    "'''\n",
    "sentence_len = 9 # batch size\n",
    "emb_dim = 10 \n",
    "num_heads = 3\n",
    "input_batch = torch.rand(sentence_len, emb_dim)\n",
    "input_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V):\n",
    "    ''' Functional implementation for scaled dot product attention formula'''\n",
    "    dot_prod = torch.matmul(Q, torch.transpose(K, -2, -1)) #swap last 2 dims, regardless of batch dim\n",
    "    K_dim = K.size(-1)\n",
    "    softmax = F.softmax(dot_prod/math.sqrt(K_dim), dim = -1)\n",
    "    attention = torch.matmul(softmax, V)\n",
    "    return attention\n",
    "\n",
    "class SelfAttentionWide(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # we want to output the same dim as embedding to enable residual connection\n",
    "        # init 3 matrices all the same\n",
    "        self.M_Q, self.M_V, self.M_K = \\\n",
    "            [nn.Linear(emb_dim, emb_dim*num_heads, bias=False) for _ in range(3)]\n",
    "        \n",
    "        self.M_merge_heads = nn.Linear(emb_dim*num_heads, emb_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # get Q, K, V\n",
    "        Q = self.M_Q(x)\n",
    "        K = self.M_K(x)\n",
    "        V = self.M_V(x)\n",
    "        multi_att = attention(Q, K, V)\n",
    "        return self.M_merge_heads(multi_att)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = SelfAttentionWide(emb_dim, num_heads)\n",
    "x = attention_layer(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0544,  0.1883, -0.0467, -0.3533,  0.0380, -0.1548, -0.0787, -0.2424,\n",
       "          0.0815, -0.0482],\n",
       "        [-0.0539,  0.1869, -0.0471, -0.3514,  0.0367, -0.1543, -0.0785, -0.2426,\n",
       "          0.0806, -0.0478],\n",
       "        [-0.0539,  0.1880, -0.0472, -0.3528,  0.0371, -0.1549, -0.0784, -0.2432,\n",
       "          0.0812, -0.0480],\n",
       "        [-0.0536,  0.1882, -0.0479, -0.3532,  0.0360, -0.1555, -0.0780, -0.2444,\n",
       "          0.0817, -0.0481],\n",
       "        [-0.0527,  0.1890, -0.0477, -0.3537,  0.0364, -0.1552, -0.0781, -0.2444,\n",
       "          0.0816, -0.0481],\n",
       "        [-0.0533,  0.1897, -0.0476, -0.3542,  0.0377, -0.1554, -0.0786, -0.2441,\n",
       "          0.0811, -0.0480],\n",
       "        [-0.0536,  0.1890, -0.0465, -0.3529,  0.0385, -0.1545, -0.0791, -0.2424,\n",
       "          0.0806, -0.0479],\n",
       "        [-0.0542,  0.1894, -0.0477, -0.3542,  0.0375, -0.1558, -0.0786, -0.2441,\n",
       "          0.0812, -0.0480],\n",
       "        [-0.0542,  0.1886, -0.0484, -0.3538,  0.0359, -0.1564, -0.0779, -0.2453,\n",
       "          0.0816, -0.0481]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0855,  1.7223,  0.1377, -1.9306,  0.7086, -0.5916, -0.0786, -1.1822,\n",
       "          1.0019,  0.1272],\n",
       "        [ 0.0901,  1.7223,  0.1362, -1.9274,  0.7040, -0.5913, -0.0768, -1.1898,\n",
       "          1.0018,  0.1309],\n",
       "        [ 0.0898,  1.7229,  0.1351, -1.9279,  0.7041, -0.5921, -0.0754, -1.1880,\n",
       "          1.0019,  0.1297],\n",
       "        [ 0.0937,  1.7229,  0.1322, -1.9259,  0.6974, -0.5932, -0.0707, -1.1921,\n",
       "          1.0052,  0.1305],\n",
       "        [ 0.0983,  1.7243,  0.1314, -1.9269,  0.6976, -0.5918, -0.0726, -1.1915,\n",
       "          1.0019,  0.1293],\n",
       "        [ 0.0940,  1.7264,  0.1322, -1.9272,  0.7049, -0.5923, -0.0763, -1.1878,\n",
       "          0.9967,  0.1295],\n",
       "        [ 0.0896,  1.7257,  0.1373, -1.9292,  0.7111, -0.5911, -0.0824, -1.1836,\n",
       "          0.9947,  0.1279],\n",
       "        [ 0.0891,  1.7257,  0.1325, -1.9267,  0.7054, -0.5937, -0.0748, -1.1870,\n",
       "          0.9989,  0.1305],\n",
       "        [ 0.0913,  1.7243,  0.1304, -1.9233,  0.6976, -0.5956, -0.0681, -1.1936,\n",
       "          1.0047,  0.1325]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layernorm = nn.LayerNorm(emb_dim)\n",
    "layernorm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, emb_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attention_layer = SelfAttentionWide(emb_dim, num_heads)\n",
    "        self.layernorm1 = nn.LayerNorm(emb_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(emb_dim)\n",
    "        # this can be pretty much any mlp we want\n",
    "        self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.ReLU())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention_layer(x)\n",
    "        x = self.layernorm1(x)\n",
    "        x = x + self.mlp(x)\n",
    "        return self.layernorm2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4111, -0.0066, -0.7764,  2.4596,  0.1680, -0.9193,  0.5762, -1.2372,\n",
       "         -0.0104, -0.6651],\n",
       "        [ 0.5130,  1.1780, -0.8943,  0.0585, -1.5254,  0.6492,  0.5785, -0.9708,\n",
       "         -1.1039,  1.5172],\n",
       "        [ 0.7809,  0.5514, -0.3827,  1.1516, -2.4058,  0.3540,  0.0678, -0.7195,\n",
       "         -0.4047,  1.0070],\n",
       "        [ 2.3102, -0.3077, -0.8528, -0.5073, -0.2442, -0.8339,  0.1017,  1.0187,\n",
       "         -1.2215,  0.5369],\n",
       "        [ 1.6129,  0.0167,  0.9968, -0.1645, -1.3619, -1.1696, -0.3388,  0.9259,\n",
       "         -1.2709,  0.7534],\n",
       "        [ 0.9896,  1.1583, -0.1301,  0.8942, -2.2386, -0.8275,  0.6987, -0.0103,\n",
       "          0.2503, -0.7846],\n",
       "        [ 1.4344, -1.3777, -0.4036,  1.5942, -0.9197, -0.3560,  0.6686, -0.7516,\n",
       "         -0.7658,  0.8770],\n",
       "        [ 1.0196, -0.5523, -0.9151,  1.5884, -1.0625,  0.6596,  0.8792,  0.6222,\n",
       "         -0.9409, -1.2982],\n",
       "        [ 0.6927, -0.3736, -1.5784, -0.8611, -1.2976,  0.7043, -0.4600,  1.1697,\n",
       "          0.5664,  1.4377]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(emb_dim, num_heads)\n",
    "encoder(input_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
