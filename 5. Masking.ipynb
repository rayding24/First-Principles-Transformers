{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It is often the case that we only want to use part of a input sequence. In the case of a sentence, we would like to \"cover\" later parts of the sentence for next token prediction, otherwise it will be trivial for the model to grab the prediction token from the input sentence. TODO: expand this on training algorithm that necessitates this, and why use -inf to achieve this, cool comparison trick to make masks; For now straight to the point -- train masked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are we having from the data??\n",
    "\n",
    "# first let's do data wrangling\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 3\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)\n",
    "\n",
    "# params and util fn for training\n",
    "bptt = 5\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, targets = get_batch(train_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   3,  158,   27],\n",
       "        [  12,    9,  123],\n",
       "        [3852,  296, 4173],\n",
       "        [3872, 8105,  243],\n",
       "        [ 884,    4,    6]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data # bsz 3, bptt 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLet's use a seq. of len. 2, after all, attention is about seq2seq\\n[x1, x2] -> attn -> [y1, y2], which should approximate ground truth [y2, y3]\\nLet's say y1 is the first to come out, it should only see x1, and y2 can see x1, x2\\nThen we need to mask out info, let's look closer\\ny1 ~ dot(k1, q1)_sm * v1 = a*v1\\ny2 ~ dot(k1, q2)_sm * v1 + dot(k2, q2)_sm * v2 = b*v1+c*v2\\nwhere ~ means proportional to, _sm means after softmaxed over all others with _sm\\nIn matrix form, it looks like:\\nY = M*V, where\\nM = \\n[[a, 0]\\n [b, c]]\\nY = [y1, y2]^T\\nV = [v1, v2]^T\\nM is row-wise softmaxed, so it means before softmax it looks like\\n[[a, -inf]\\n [b, c]]\\nNow we see clearly where the mask comes in -- right before the softmax step, then it's business as usual\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It can get very confusing why we use a triangular shaped mask\n",
    "# but it really is very simple once we go to first principles\n",
    "'''\n",
    "masking -- the artifect of matrix notation to capture the vector sums\n",
    "Let's use a seq. of len. 2, after all, attention is about seq2seq\n",
    "[x1, x2] -> attn -> [y1, y2], which should approximate ground truth [y2, y3]\n",
    "Let's say y1 is the first to come out, it should only see x1, and y2 can see x1, x2\n",
    "Then we need to mask out info, let's look closer\n",
    "y1 ~ dot(k1, q1)_sm * v1 = a*v1\n",
    "y2 ~ dot(k1, q2)_sm * v1 + dot(k2, q2)_sm * v2 = b*v1+c*v2\n",
    "where ~ means proportional to, _sm means after softmaxed over all others with _sm\n",
    "In matrix form, it looks like:\n",
    "Y = M*V, where\n",
    "M = \n",
    "[[a, 0]\n",
    " [b, c]]\n",
    "Y = [y1, y2]^T\n",
    "V = [v1, v2]^T\n",
    "M is row-wise softmaxed, so it means before softmax it looks like\n",
    "[[a, -inf]\n",
    " [b, c]]\n",
    "Now we see clearly where the mask comes in -- right before the softmax step, \n",
    "then it's business as usual\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(Q, K, V, mask=None): \n",
    "    ''' Functional implementation for scaled dot product attention formula with optional binary mask'''\n",
    "    dot_prod = torch.matmul(Q, torch.transpose(K, -2, -1)) #swap last 2 dims, regardless of batch dim\n",
    "    K_dim = K.size(-1)\n",
    "    if mask:\n",
    "        dot_prod[mask] = float('-inf') # proper mask for batch?\n",
    "    softmax = F.softmax(dot_prod/math.sqrt(K_dim), dim = -1)\n",
    "    attention = torch.matmul(softmax, V)\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need input batch in correct dim and verify that the output is correct\n",
    "# let's engineer an example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
