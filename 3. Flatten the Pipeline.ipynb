{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first let's do data wrangling\n",
    "\n",
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train_txt)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'\n",
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data.examples[0].text])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "train_data = batchify(train_txt, batch_size)\n",
    "val_data = batchify(val_txt, eval_batch_size)\n",
    "test_data = batchify(test_txt, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params and util fn for training\n",
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target\n",
    "\n",
    "ntokens = len(TEXT.vocab.stoi) # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "# model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't want to bother too much with pos encoding so we just use the class \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's train for one batch\n",
    "ntokens = len(TEXT.vocab.stoi)\n",
    "data, targets = get_batch(train_data, 0)\n",
    "# make mask\n",
    "sz = bptt\n",
    "mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "# parts for the model\n",
    "ninp = emsize\n",
    "pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "encoder = nn.Embedding(ntokens, ninp)\n",
    "decoder = nn.Linear(ninp, ntokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run through model\n",
    "src = data\n",
    "src1 = encoder(src) * math.sqrt(ninp)\n",
    "src2 = pos_encoder(src1)\n",
    "output0 = transformer_encoder(src2, mask)\n",
    "output = decoder(output0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3) tensor([ -0.5931,  14.1556,  11.5772,   7.9735,   5.9866,   5.9855,   8.2021,\n",
      "        -25.7343, -22.5729], grad_fn=<SliceBackward>)\n",
      "torch.Size([35, 20]) torch.Size([35, 20, 200])\n",
      "torch.Size([35, 20, 28785]) torch.Size([700])\n"
     ]
    }
   ],
   "source": [
    "print(src[0][0], src1[0][0][:9]) # dict value vs. embedding of value :9 is to effect only, it's really of emsize=200 vec\n",
    "print(src.size(), src1.size())\n",
    "print(output.size(), targets.size()   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.3970, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = criterion(output.view(-1, ntokens), targets)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    3,    25,  1849,   570,     7,     5,     5,  9258,     4,    56,\n",
       "             0,     7,     6,  6634,     4,  6603,     6,     5,    65,    30],\n",
       "        [   12,    66,    13,  4889,   458,     8,  1045,    21, 19094,    34,\n",
       "           147,     4,     0,    10,  2280,  2294,    58,    35,  2438,  4064],\n",
       "        [ 3852, 13667,  2962,    68,     6, 28374,    39,   417,     0,  2034,\n",
       "            29,    88, 27804,   350,     7,    17,  4811,   902,    33,    20],\n",
       "        [ 3872,     5,     9,     4,   155,     8,  1669,    32,  2634,   257,\n",
       "             4,     5,     5,    11,  4568,  8205,    78,  5258,  7723, 12009],\n",
       "        [  884,    91,   963,   294,     4,   548,    29,   279,    37,     4,\n",
       "           391,    31,     4,  2614,   948, 13583,   405,   545,    15,    16],\n",
       "        [   12,    25,     5,     5,  1688,     0,    39,    59,  8785,     0,\n",
       "             6,    13,  3026,    43,    11,     6,     0,   349,  3134,  4538],\n",
       "        [    3,     6,    82,  1780,    21,     6,  2158,     4,     8,     8,\n",
       "            27,  1485,     0,   194,    96,   195,  3545,   101,  1150,  3486],\n",
       "        [    3,    25,    13,   885,     4,  6360,    15,   670,     0,    13,\n",
       "            26,    17,     5,   417,   894,    10,     5,     5,  2998,    27],\n",
       "        [20003,   190,    33,  1516,  1085,    34,   680,  3597,  2475,   664,\n",
       "            47,    11,   127,    63,     6,    46, 24995,    72, 10190,    26],\n",
       "        [   86,  9076, 10540,     6,     9,    74,   198,     7,     6,    17,\n",
       "          3134,  5312,     4,     4,     3, 25509,     5,  2034,     5,    86],\n",
       "        [ 3852,     5,     7,    48,  4870,   362,     6,   840,    45,    33,\n",
       "            21,     9,  4379,   183,    11,     5,     0,     9,   775,     0],\n",
       "        [   91,     8,   293,  1857,     5,    28,    16,     5,  2705,  1130,\n",
       "             0,    11,   615,   317,  4648,   543,     5,   361,    22,    10],\n",
       "        [    0,    66,  1933,    22,   104,    49,     4,   133,   171,     8,\n",
       "            50, 12127,    10,     6,  3683, 16138, 13801,     8, 14548,    11],\n",
       "        [ 3872,    25,    28,  5775,    64,    40,    98,     4,    25,  6583,\n",
       "         28555, 14110, 17910,     9,    22,     8,     7,   300,    29,   757],\n",
       "        [   24,     6,  5662,  6708,  2849,  1146,   234,  1092,     6,  4909,\n",
       "             5,     8,    11,     4,    49,  8545,     4,     6,   163,    10],\n",
       "        [  783,    25,     6,    11,    52,     5,     7,   454,    25,     7,\n",
       "          1183,  2459,  1684,   561,  2033,     7,  7582,     4,   276,  1556],\n",
       "        [28783,   112,     4,    98,    54,    35,     4,    26,   194,     4,\n",
       "           511,    11,   705,     5,     7, 11072,     8,    72,   180,    19],\n",
       "        [    5,  8088,   334,    61,   792,    13,  1669,    28,    43,   186,\n",
       "             7,   386,   923,     0, 15443,    60, 18920,  1311,     6,    30],\n",
       "        [ 6185,   471,     7,     5,     5, 12612,     5,     4,    66, 23025,\n",
       "             0,  1321,     5,  1232,     8,  2609,     5,  1992,  1193,  4064],\n",
       "        [    6,    70,  4014,  4135,   669, 16856,  7946,  1650,    25,     9,\n",
       "             5,   562,     8,   555, 27230,     5,     0,     7,   148,    26],\n",
       "        [ 3852,     6,    13,   385, 16882,     5, 10562,   134,     6,  1325,\n",
       "            11,     4,    13,    16,     5,     4,     8,  2525,   450,    20],\n",
       "        [    7,    31,  1623,  5072,    28, 16585,   552,     6,    25,     6,\n",
       "           663,   596,     4,    11,    56,  1408, 28426,    13,  3297,     4],\n",
       "        [    4,    47,     0,   570,  2420,     5,    33,  3629,    91,     9,\n",
       "           295,   198, 26410,  1509,   516,    13,     6,     9,    26,   261],\n",
       "        [ 5026,   794,    22,  4889,     8,     8,   359,  4393,  1016,    78,\n",
       "            10,     6,  6797,    15,  1069,  3488,     0,  2585,    19,  1111],\n",
       "        [   91,  1058,  6409,     5,   425,     0,    15,    13,    24,  2474,\n",
       "          6404,    59,    29,   680,    11,  2767,     5,     5,    67,   628],\n",
       "        [   23,   180,     9,    35,   595,     6,   680,    11,   171,     5,\n",
       "             6,    30,     4,  1862,  5475,    20, 18475,     8,  1017,    84],\n",
       "        [    5,    28,  6097,   513,     6,  4775,  1072,  3042,    25,    10,\n",
       "             0,   402,   704,   339,    21,     4,     8,     4,    11,    20],\n",
       "        [ 1840,     4,    50,   885,     3, 10567,    10,   161,     6,  1783,\n",
       "             5,     9,   155,    10,  6318,  7274,     0,   295,   878,     4],\n",
       "        [ 1021,   373,  9433,   535,     3,  1597, 26809,   279,    25,  6841,\n",
       "           238,     0,     4,   297,  7606,     8,    78,   200,  1060,   187],\n",
       "        [   10,     5,     6,  1516,    12,    97, 23594,    16,    73, 18558,\n",
       "            10,     5,  1526,     4,    24,    39,    36,    36,    84,     6],\n",
       "        [   17,     8,     9,     6,    12,     9,     5,   644,    43,     0,\n",
       "             0,     9,   705,   183,  2219,  7429,  1966,     9,  3524,     6],\n",
       "        [ 3852,  4312,  9049,     3,   942,   197,  3077,  9191,   171,    24,\n",
       "          1687,   382,     6,   945,     0,     6,     6,     0,  3163,     6],\n",
       "        [ 3872,   180,     5,    16,    12,   588,   121,     5,    25,   971,\n",
       "             5,  2089,     3,  3857,    23,    46,    82,     4,   150,   182],\n",
       "        [  884,    28,    27,   435,    12,     6,    63,   133,     6,     0,\n",
       "            13,    31,     3,    43,     8,  2997,    78,  5977,    52,   181],\n",
       "        [  632,     4,   127,     6,     3,  4775,     4,     4,    25,    23,\n",
       "           124,  5223,    12,   194,     4,     8,    36,  2142,   139,    37]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  6.8754,  -9.4439,  11.5488,  ...,   6.9110,  -8.7892, -27.1390],\n",
       "         [-15.1194, -12.6068, -22.7539,  ...,  -6.8978,  -8.8547,  -9.9066],\n",
       "         [ -1.7704, -14.6749,   9.8628,  ...,  -3.7070,  -3.8328, -19.8940],\n",
       "         ...,\n",
       "         [  8.4953, -38.1183,  -8.1167,  ...,  -2.1501,  -6.3575, -25.0811],\n",
       "         [  8.8252,  -3.5102,  12.5300,  ...,  -6.9726,  18.4786,  15.9362],\n",
       "         [-15.8234,  -3.1877, -16.3614,  ...,  -4.7671,   9.4257,   3.5041]],\n",
       "\n",
       "        [[  3.6808,  18.5054,  -4.5577,  ..., -21.9113,  -3.3585,  -7.6830],\n",
       "         [  2.3326, -35.8592,  12.5238,  ...,  -7.0262, -11.6627,  -0.2249],\n",
       "         [-43.0223, -22.6506,  22.6495,  ...,  -8.3481,  -7.5752,   4.0078],\n",
       "         ...,\n",
       "         [  2.9205,   6.9040,   2.5209,  ...,  -2.8129, -16.2818,   3.7259],\n",
       "         [ -3.8394,   6.7017, -14.7165,  ...,  17.8833, -20.8497,   2.0899],\n",
       "         [ 18.9062,  10.7315,  24.1147,  ...,  24.6830,  21.8114, -21.2592]],\n",
       "\n",
       "        [[ 14.7675,   9.5921,  33.8792,  ...,  -0.9379, -18.8451,  12.3550],\n",
       "         [  2.4257,  -0.5477,  30.8960,  ...,  21.4442,  -8.0514,  23.2124],\n",
       "         [-16.4681,  18.1286, -14.7185,  ...,   9.8381,  15.2157,  -7.9782],\n",
       "         ...,\n",
       "         [ -9.2945,  10.6705, -14.3496,  ...,   4.3291, -21.3961,   3.7425],\n",
       "         [-14.3830,   6.6468,  -7.1757,  ..., -40.3322,  -2.5841,  -0.4320],\n",
       "         [-10.1136,   5.5712,   4.2016,  ...,   1.3740,  11.2804,  14.3582]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  4.7765,   4.2908,   0.4660,  ...,   2.0418,  -0.8221, -11.6627],\n",
       "         [ 10.7452,  10.2407,  10.4760,  ...,   2.2660,   0.1759,  29.0105],\n",
       "         [  8.4953, -38.1183,  -8.1167,  ...,  -2.1501,  -6.3575, -25.0811],\n",
       "         ...,\n",
       "         [-19.1827, -20.0709,  -7.0962,  ...,  -1.4252, -17.7963,   3.2381],\n",
       "         [ -2.4128, -14.1370,  20.0821,  ...,   0.1263,   5.6313,  -0.2528],\n",
       "         [-15.9395,  -9.9622,  28.9386,  ...,  -7.3479,  -5.4633,   4.0372]],\n",
       "\n",
       "        [[  0.9646,  -4.9829,   7.2807,  ...,   0.3690,  -2.9032,  -4.1831],\n",
       "         [ -5.0784,   0.6923, -15.2018,  ...,  23.1964,   9.0182,  -8.0449],\n",
       "         [  8.6676,  -5.1830,  13.5630,  ...,  22.9662,   2.8267,   7.2330],\n",
       "         ...,\n",
       "         [ 11.4988,   4.1636, -18.0620,  ..., -13.1018, -23.0793,  -9.1765],\n",
       "         [ 13.1995, -11.6535, -10.2587,  ...,  19.9191,  13.3264,  -6.8441],\n",
       "         [ 12.3808,  -6.9090, -20.5371,  ...,   7.1010,   6.6029,   0.1389]],\n",
       "\n",
       "        [[  3.8628,  -8.8481, -15.4480,  ...,  10.9916, -13.1543,  -6.9258],\n",
       "         [-19.1827, -20.0709,  -7.0962,  ...,  -1.4252, -17.7963,   3.2381],\n",
       "         [  6.1726,  -9.7521,  -6.1521,  ...,  12.9811, -31.2159,  -0.4675],\n",
       "         ...,\n",
       "         [-21.4681, -15.9640,  17.5532,  ..., -12.8035,  -1.9583,  17.0110],\n",
       "         [-18.5922,  16.5769,   7.6203,  ...,   8.6247,   8.9763,  -2.6883],\n",
       "         [ -5.6796,   8.8190, -28.8854,  ..., -24.6153,   0.8710,  18.0714]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1061, -0.9796,  0.9826,  ...,  1.1279,  0.5106, -1.7847],\n",
       "         [-1.1990,  0.2679, -1.9748,  ..., -0.5389,  0.2254, -0.5904],\n",
       "         [ 0.0860, -0.2968, -0.1216,  ...,  0.1845, -0.0208, -0.2638],\n",
       "         ...,\n",
       "         [ 0.5171, -2.6012, -0.7301,  ..., -0.7282, -0.4032, -1.4576],\n",
       "         [ 1.2383,  0.6211,  0.9330,  ..., -0.7069,  1.4402,  0.2654],\n",
       "         [-1.1168, -0.2211, -0.6438,  ...,  0.3666,  1.4219,  0.2521]],\n",
       "\n",
       "        [[ 0.3981,  0.4132, -0.1645,  ..., -0.3574,  0.3433, -1.0266],\n",
       "         [-0.4795, -1.6399,  0.3169,  ..., -0.2813,  0.1042,  0.5923],\n",
       "         [-2.4172, -1.2791,  2.0421,  ..., -0.2753,  0.7108,  0.2609],\n",
       "         ...,\n",
       "         [ 0.0651,  0.8549, -0.5745,  ..., -2.1115, -0.9226,  0.2294],\n",
       "         [ 0.4881,  0.1432, -1.7524,  ..., -0.0991, -0.8505, -0.1755],\n",
       "         [ 0.6100,  1.4778,  1.2633,  ...,  2.1395,  1.4001, -1.9058]],\n",
       "\n",
       "        [[ 1.1407,  1.1470,  2.2177,  ..., -0.0670, -1.4174,  0.7088],\n",
       "         [ 0.1723,  0.2673,  1.8657,  ...,  1.8927, -0.7377,  1.3615],\n",
       "         [-1.0682,  1.0035,  0.6456,  ...,  0.4266,  0.2930, -1.5538],\n",
       "         ...,\n",
       "         [-0.7660,  1.4054, -0.5751,  ..., -0.0764, -2.3572,  0.6869],\n",
       "         [-1.1722,  0.6705, -0.3189,  ..., -2.2323, -0.6073, -0.2260],\n",
       "         [-0.3284,  0.1647, -0.3443,  ...,  0.0440,  0.7013,  0.9997]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 1.4659,  0.0507, -0.1275,  ..., -1.1996,  0.0311, -0.6824],\n",
       "         [ 0.1828,  0.6110,  1.0383,  ...,  0.1067,  0.2830,  2.2780],\n",
       "         [ 0.2418, -3.2158, -0.7688,  ..., -0.7635, -0.9231, -2.6174],\n",
       "         ...,\n",
       "         [-1.0316, -1.3674, -0.6109,  ..., -0.9969, -1.0195, -0.5319],\n",
       "         [ 0.0607, -1.0800,  0.6638,  ..., -0.3777,  0.2763,  1.1240],\n",
       "         [-1.7074, -0.5280,  2.1243,  ..., -1.1506,  0.0419,  1.3670]],\n",
       "\n",
       "        [[-0.2389, -0.3131,  1.3838,  ...,  0.1073,  1.2937,  1.0167],\n",
       "         [-0.1093,  1.0295, -0.8216,  ...,  0.6179,  0.2860, -1.5427],\n",
       "         [-0.1962, -0.3756,  1.1187,  ...,  1.0873,  0.2594,  0.0148],\n",
       "         ...,\n",
       "         [ 1.1652,  0.4317, -1.1550,  ..., -1.5452, -1.1590, -0.5933],\n",
       "         [ 0.6233, -0.9331, -0.5295,  ...,  0.6922,  0.8358, -0.5996],\n",
       "         [ 0.0128, -0.7943, -1.9857,  ...,  0.3244,  0.9295,  1.3066]],\n",
       "\n",
       "        [[-0.1522, -0.9640, -0.3852,  ...,  0.7368, -0.8638, -0.1549],\n",
       "         [-1.3533, -1.3450, -0.5582,  ..., -0.4515, -1.0185,  1.0470],\n",
       "         [-0.7103, -0.7937, -0.7122,  ...,  0.8907, -1.5409, -0.1475],\n",
       "         ...,\n",
       "         [-1.2627, -1.1375,  0.9582,  ..., -0.7823,  0.5255,  0.7314],\n",
       "         [-1.4260,  2.0962,  0.7111,  ...,  0.5538,  0.7976, -0.2792],\n",
       "         [-0.3640,  0.8566, -1.6156,  ..., -1.7149,  0.5642,  1.2508]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layers(src1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35, 20, 200])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layers(src1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
