{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "spread-prayer",
   "metadata": {},
   "source": [
    "To talk about masking one must have training/inference in mind, the GPT models are autoregressive, which is pretty much just a fancy way of saying, \"give it a few words/tokens, let it predict the next one\". The huge scaling benefit of this lies in the unsupervised nature (no labels needed, to wit, \"training data is also test data\") allows autoregressive models to train on any amount of text data available, contents from the whole internet is the limit (and has indeed been attempted: https://commoncrawl.org/).  \n",
    "\n",
    "Masking is just a technicality to ensure that the autoregressive property holds by not allowing the model to look ahead. While it is not a deep concept, its implementation can be a source of confusion. So let's get the questions-party started.\n",
    "\n",
    "There can be other masks for different needs (e.g. BERT). We focus on the autoregressive one here. \n",
    "\n",
    ">   Q1. Why would the model look ahead? \n",
    "\n",
    "This is due to the training process and the very heart of attention (processing many tokens at once). Now picture or refer to a diagram for the attention mechanism. It receives a input sequence of n vectors, each is called a token, representing a word. It outputs n vectors, which eventually will be transformed into n predictions for the next-token, for which the \"ground truth\" should be each word's next neighhour in the input sequence, i.e. the solution is right in the input. It is obvious that we will reveal the ground truth for attention to overfit on if we let it see the whole sequence, so we must have a way to hide the solution so the model cannot cheat -- since during inference we cannot supply the next tokens.\n",
    "\n",
    "Another way to look at this is that the feature of attention layer that enables it to take in multiple tokens at once (a key advantage over RNNs) means we need something to preserve causality which RNN naturally does, and the trick we use is masking.\n",
    "\n",
    ">   Q2. So what do we do?\n",
    "\n",
    "We add an extra step inside attention so that the i th output token can only receive information from 0 to i th (inclusive) input tokens.\n",
    "\n",
    ">   Q3. What extra step? And why is it positioned right before softmax? And why is the mask a triangular matrix? $-\\infty$?\n",
    "\n",
    "Actually, the core logic is almost trivial if we recast this into a translation issue from vector to matrix notion. \n",
    "\n",
    "Masking -- the artifect of matrix notation to capture the vector sums.\n",
    "\n",
    "The logic with summuing vectors to preserve causality is almost trivial, it's only when we want to translate this into matrix notation that it begins to look like an advanced trick, but if we view it this way, the core logic is as trivial as saying \"drop any info from future tokens\", or \"if (model about to cheat): don't. \"\n",
    "\n",
    "Let's use a seq. of len. 2 to illustrate the process. Attention is really just about seqeunce to sequence which looks like:\n",
    "\n",
    "[x1, x2] -> attention layer -> [y1, y2]\n",
    "\n",
    "Let's say y1 is the first to come out, it should only see x1, and y2 should be able to see x1, x2 as discussed above. \n",
    "\n",
    "Then we need to mask out info, let's look closer\n",
    "\n",
    "If we write attention as a sum\n",
    "\n",
    "$$y_1 = w_1 \\cdot v_1 \\\\ \n",
    "\\sim \\exp(k_1 \\cdot q_1)  v_1\\\\\n",
    "= \\exp(k_1 \\cdot q_1)  v_1 + 0 \\cdot v_2 \\\\\n",
    "= exp(k_1 \\cdot q_1)  v_1 + \\exp(-\\infty) \\cdot v_2 $$\n",
    "\n",
    "$$y_2 = w_3 \\cdot v_1+ w_4 \\cdot v_2 \\\\\n",
    "\\sim \\exp(k_1 \\cdot q_2)  v_1 + \\exp(k_2 \\cdot q_2)  v2 $$\n",
    "\n",
    "where ~ means proportional to since we don't need to other constants to illustrate the point, and $w_i$ are weights for matrices below.\n",
    "\n",
    "At the point, we ask, how do we turn this into matrix operation? \n",
    "\n",
    "In matrix form, it looks like:\n",
    "\n",
    "Y = W*V\n",
    "\n",
    "where,\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "w_1 & 0\\\\\n",
    "w_3 & w_4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    " \n",
    "$$ Y = \\begin{bmatrix} y_1 \\\\ y_2  \\end{bmatrix}$$\n",
    "\n",
    "$$ V = \\begin{bmatrix} v_1 \\\\ v_2  \\end{bmatrix}$$\n",
    "\n",
    "Now, we know M is row-wise softmaxed, so it means before softmax it must look like,\n",
    "\n",
    "$$ W_{raw} = \\begin{bmatrix}\n",
    "w_{raw1} & -\\infty\\\\\n",
    "w_{raw3} & w_{raw4}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    " \n",
    "This is why we want to apply a \"mask\" to make the elements above diagonal in $W_{raw}$ to be $-\\infty$, which will translate into dropping any vectors derived from future tokens in matrix notation.\n",
    "\n",
    "Now we see clearly where the mask comes in -- right before the softmax step, \n",
    "everything else is business as usual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "powered-software",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's demonstrate the masking in action\n",
    "#first we start as usual until the softmax step\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "emb_dim = 4\n",
    "seq_len = 5\n",
    "\n",
    "#input tokens:\n",
    "x = torch.tensor([0,1,2,3,4]) \n",
    "embedding = nn.Embedding(20, emb_dim) #vocab size 20, emb dim 4\n",
    "x = embedding(x)\n",
    "\n",
    "#attention matrices\n",
    "M_K, M_Q, M_V = [nn.Linear(emb_dim, emb_dim, bias=False) for _ in range(3)]\n",
    "K, Q, V = [M(x) for M in [M_K, M_Q, M_V ]]\n",
    "W_raw = Q@(K.transpose(-1,-2))\n",
    "\n",
    "# stop before next steps\n",
    "# W = F.softmax(W_raw, dim=-2)\n",
    "# Y = W@V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "suspended-auction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7918, -0.5124,  0.7467, -0.1884, -0.7821],\n",
       "        [-0.0497, -0.0079, -0.2227,  0.0234,  0.1894],\n",
       "        [-0.9871,  0.3926,  0.0859,  0.3219,  0.9626],\n",
       "        [-0.3943,  0.1961, -0.0807,  0.1332,  0.4240],\n",
       "        [-0.9710,  0.5766, -0.3295,  0.3093,  0.9815]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "sticky-motorcycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 1, 1, 1],\n",
       "        [0, 0, 1, 1, 1],\n",
       "        [0, 0, 0, 1, 1],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 5\n",
    "ones = torch.ones((seq_len, seq_len), dtype=torch.uint8)\n",
    "mask = torch.triu(ones, diagonal=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "comic-component",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-e315cefb68bb>:1: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  W_raw[mask] = float('-inf')\n"
     ]
    }
   ],
   "source": [
    "# in Pytorch we can apply mask with another tensor as boolean \n",
    "# i.e. if mask[i][j] == 1: W_raw[i][j] = float('-inf')\n",
    "W_raw[mask] = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "serial-postage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7918,    -inf,    -inf,    -inf,    -inf],\n",
       "        [-0.0497, -0.0079,    -inf,    -inf,    -inf],\n",
       "        [-0.9871,  0.3926,  0.0859,    -inf,    -inf],\n",
       "        [-0.3943,  0.1961, -0.0807,  0.1332,    -inf],\n",
       "        [-0.9710,  0.5766, -0.3295,  0.3093,  0.9815]],\n",
       "       grad_fn=<IndexPutBackward>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "desperate-distribution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4896, 0.5104, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1266, 0.5032, 0.3702, 0.0000, 0.0000],\n",
       "        [0.1704, 0.3076, 0.2332, 0.2888, 0.0000],\n",
       "        [0.0548, 0.2576, 0.1041, 0.1972, 0.3862]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# proceed with the next steps to finish attention as usual\n",
    "W = F.softmax(W_raw, dim=-1)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "purple-toddler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7833,  0.4562,  0.3547,  0.4063],\n",
       "        [ 0.2458, -0.4403,  0.2678, -0.7130],\n",
       "        [-0.0089, -0.5916,  0.7337, -0.9883],\n",
       "        [ 0.1896, -0.2253,  0.1491, -0.5264],\n",
       "        [ 0.6331, -0.2735, -0.1155, -0.8580]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "stylish-softball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7833,  0.4562,  0.3547,  0.4063],\n",
       "        [-0.2580, -0.0014,  0.3103, -0.1651],\n",
       "        [ 0.0212, -0.3828,  0.4513, -0.6732],\n",
       "        [-0.0052, -0.2607,  0.3570, -0.5326],\n",
       "        [ 0.3014, -0.3001,  0.1496, -0.6995]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = W@V\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "colonial-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To summarize, we can write the masked version of attention as:\n",
    "\n",
    "def self_attention_with_mask(x, emb_dim, seq_len):\n",
    "    M_K, M_Q, M_V = [nn.Linear(emb_dim, emb_dim, bias=False) for _ in range(3)]\n",
    "    K, Q, V = [M(x) for M in [M_K, M_Q, M_V ]]\n",
    "    W_raw = Q@(K.transpose(-1,-2))\n",
    "    # == masking begins ==\n",
    "    ones = torch.ones((seq_len, seq_len), dtype=torch.uint8)\n",
    "    mask = torch.triu(ones, diagonal=1)\n",
    "    W_raw[mask] = float('-inf')\n",
    "    # == masking ends ==\n",
    "    W = F.softmax(W_raw, dim=-1)\n",
    "    Y = W@V\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "indie-contractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-47-442fd643b0fd>:10: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "  W_raw[mask] = float('-inf')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7491,  0.7891,  0.0687, -0.4984],\n",
       "        [-0.2918,  0.6358, -0.1248, -0.1398],\n",
       "        [-0.4738,  0.8256, -0.1297, -0.2850],\n",
       "        [ 0.0148,  0.7448, -0.4092,  0.0290],\n",
       "        [ 0.1783,  0.6184, -0.4607,  0.0386]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attention_with_mask(x, emb_dim, seq_len) \n",
    "#result will look different each time due to re initialization of nn.Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-stereo",
   "metadata": {},
   "source": [
    "Next, we will move on to constructing a transformer and the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-transcription",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
