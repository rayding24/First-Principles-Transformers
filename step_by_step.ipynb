{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SO', 'to', 'begin', 'with', 'I', 'have', 'a', 'sentence']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SO to begin with I have a sentence\n",
    "sentence = 'SO to begin with I have a sentence'.split(' ')\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SO': 0,\n",
       " 'to': 1,\n",
       " 'begin': 2,\n",
       " 'with': 3,\n",
       " 'I': 4,\n",
       " 'have': 5,\n",
       " 'a': 6,\n",
       " 'sentence': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to convert sentence to numbers before converting to tensor\n",
    "word2idx = {}\n",
    "count = 0\n",
    "for word in sentence:\n",
    "    if word in word2idx:\n",
    "        continue\n",
    "    else:\n",
    "        word2idx[word] = count\n",
    "        count += 1\n",
    "    \n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "tensor([[[ 0.6287,  1.6665, -0.3331, -0.4593, -1.6786],\n",
      "         [ 0.2869, -0.0948, -0.3191, -0.9615,  2.1615],\n",
      "         [ 1.3007,  0.2417, -1.2106, -2.0156,  1.5267],\n",
      "         [ 1.5645, -0.7544,  0.0069, -1.7901, -1.2235],\n",
      "         [ 1.9531, -0.7038,  0.3065, -1.0574, -0.7996],\n",
      "         [ 1.6464,  0.4403,  0.4414, -0.1570, -0.3825],\n",
      "         [-0.3843, -0.6210, -0.1699,  0.5268, -1.1570],\n",
      "         [-1.3945, -0.3285, -0.6545, -0.3870, -0.5344]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# then there was some kind of embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[word2idx[word] for word in sentence]]) # batch 1 for sentence\n",
    "print(x)\n",
    "embed = nn.Embedding(20, 5) #vocab 20 and vec size 5\n",
    "x = embed(x)\n",
    "print(x)\n",
    "# so each word -> number -> vector, sure, that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there was this positional encoding thing, but let's skip it since it's added, ie assume 0\n",
    "# then we got multiheaded attention\n",
    "# So attention can be seen as this heuristic:\n",
    "# IN ESSENCE -- add more changeable parameters/degree of freedom to the most BASIC approach\n",
    "# 1. We want seq -> seq, and somehow the output seq captures the correlations with input seq\n",
    "# 2. A natural way to do this is just weighted sum\n",
    "# 3. A natural way for weights is just dot products\n",
    "# 4. To make things easier, we normalize, with softmax\n",
    "# 5. The raw vectors may not be in the right latent space, let's add an extra linear transf.\n",
    "# 6. AND, let each 'raw vec term' in the weighted sum have its own customizable projection\n",
    "# 7. Voila! These transformed raw vectors are key, quary and values! \n",
    "#    And we get the output seq by good old weighted sum (weights as dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 5\n",
    "latent_dim = 7\n",
    "#init the matrices to get K, Q, V vecotors\n",
    "M_K, M_Q, M_V = [torch.rand(emb_dim, latent_dim) for _ in range(3)]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3447, 0.8092, 0.3961, 0.4156, 0.9257, 0.4706, 0.7407],\n",
       "        [0.5121, 0.5361, 0.9129, 0.5644, 0.2540, 0.2610, 0.1731],\n",
       "        [0.6374, 0.6320, 0.2554, 0.5370, 0.6227, 0.6127, 0.1856],\n",
       "        [0.2040, 0.9485, 0.3082, 0.9250, 0.6534, 0.0748, 0.3550],\n",
       "        [0.2369, 0.1740, 0.7922, 0.8342, 0.0204, 0.1259, 0.6711]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3665,  0.4639,  0.2141, -0.8021,  0.4635,  0.2811, -0.5971],\n",
       "         [ 0.1629, -0.5562,  1.3616,  0.8082, -0.5413,  0.1150,  1.2462],\n",
       "         [-0.2491, -1.2292,  1.0150, -0.5639, -0.7743, -0.0252,  1.0896],\n",
       "         [-0.4977, -1.0448, -1.5880, -2.4483,  0.0663,  0.2555, -0.4271],\n",
       "         [ 0.1031,  0.2549, -0.7498, -1.0659,  1.1129,  0.7434,  0.4696],\n",
       "         [ 0.9517,  1.6318,  0.8155,  0.7055,  1.8004,  1.1003,  1.0651],\n",
       "         [-0.7254, -0.4530, -1.5167, -1.0794, -0.2987, -0.5533, -1.0132],\n",
       "         [-1.2717, -2.1783, -1.5621, -1.9204, -2.0457, -1.2393, -1.7073]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@M_K #this is how one transforms the whole batch of words!\n",
    "# the trick to applying matrix to a batch is really just put x first \n",
    "# it can be hard to make that mental switch from the math background\n",
    "# since in math we are so used to put the matrix before x, but nothing special is going on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "K, Q, V = x@M_K, x@M_Q, x@M_V \n",
    "# to get all pairs of dot products between Q_i and K_j, we multiply the \"batch matrices\" like so\n",
    "# here matrices is truly a bookkeeping device\n",
    "# and the matrix multiplication here is just a neat notation to get all pairs\n",
    "# almost like list comprehension is a neat notation for for loop, the essence remains basic\n",
    "# W_ij = Q_i . K_j\n",
    "W_raw = Q@(K.transpose(1,2))\n",
    "# when we do the final sum, we are summing over (or contracting the index of;) j -- the keys\n",
    "# and we softmax these weights, i.e softmax all the rows of W\n",
    "W = F.softmax(W_raw, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check softmax does indeed makes probabilities\n",
    "torch.sum(W, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4231, -0.6297, -0.1127, -0.1433, -0.3924,  0.1836, -0.0788],\n",
       "         [ 0.2189, -0.1688,  0.0991,  0.1640, -0.1528,  0.0889, -0.0864],\n",
       "         [ 0.6795, -0.8617,  0.1346,  0.2751, -0.7951,  0.1563, -0.5765],\n",
       "         [ 0.4446, -0.6852, -0.0218, -0.0228, -0.5302,  0.0906, -0.3964],\n",
       "         [ 0.4071, -0.5548,  0.0055, -0.0293, -0.3627,  0.1148, -0.2745],\n",
       "         [ 2.1575, -0.5151,  1.7189,  1.3461,  0.4325,  1.2230,  0.4569],\n",
       "         [ 0.0795, -0.1965, -0.0770, -0.0895, -0.1347,  0.0139, -0.0796],\n",
       "         [-2.4305, -3.1821, -4.7890, -5.0264, -2.9587, -2.2738, -3.3613]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting it together: y_i = \\sum_j{ W_i_j * v_j}\n",
    "# in matrix form -- one could check by hand, it is\n",
    "Y = W@V\n",
    "# so yeah, batching can use matrix operations to bookkeep things and results elegantly!\n",
    "Y\n",
    "# num_words x latent dim -- shape checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
