{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SO', 'to', 'begin', 'with', 'I', 'have', 'a', 'sentence']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SO to begin with I have a sentence\n",
    "sentence = 'SO to begin with I have a sentence'.split(' ')\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SO': 0,\n",
       " 'to': 1,\n",
       " 'begin': 2,\n",
       " 'with': 3,\n",
       " 'I': 4,\n",
       " 'have': 5,\n",
       " 'a': 6,\n",
       " 'sentence': 7}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to convert sentence to numbers before converting to tensor\n",
    "word2idx = {}\n",
    "count = 0\n",
    "for word in sentence:\n",
    "    if word in word2idx:\n",
    "        continue\n",
    "    else:\n",
    "        word2idx[word] = count\n",
    "        count += 1\n",
    "    \n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2, 3, 4, 5, 6, 7]])\n",
      "tensor([[[ 0.6287,  1.6665, -0.3331, -0.4593, -1.6786],\n",
      "         [ 0.2869, -0.0948, -0.3191, -0.9615,  2.1615],\n",
      "         [ 1.3007,  0.2417, -1.2106, -2.0156,  1.5267],\n",
      "         [ 1.5645, -0.7544,  0.0069, -1.7901, -1.2235],\n",
      "         [ 1.9531, -0.7038,  0.3065, -1.0574, -0.7996],\n",
      "         [ 1.6464,  0.4403,  0.4414, -0.1570, -0.3825],\n",
      "         [-0.3843, -0.6210, -0.1699,  0.5268, -1.1570],\n",
      "         [-1.3945, -0.3285, -0.6545, -0.3870, -0.5344]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "# then there was some kind of embedding\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x = torch.tensor([[word2idx[word] for word in sentence]]) # batch 1 for sentence\n",
    "print(x)\n",
    "embed = nn.Embedding(20, 5) #vocab 20 and vec size 5\n",
    "x = embed(x)\n",
    "print(x)\n",
    "# so each word -> number -> vector, sure, that works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now there was this positional encoding thing, but let's skip it since it's added, ie assume 0\n",
    "# then we got multiheaded attention\n",
    "# So attention can be seen as this heuristic:\n",
    "# IN ESSENCE -- add more changeable parameters/degree of freedom to the most BASIC approach\n",
    "# 1. We want seq -> seq, and somehow the output seq captures the correlations with input seq\n",
    "# 2. A natural way to do this is just weighted sum\n",
    "# 3. A natural way for weights is just dot products\n",
    "# 4. To make things easier, we normalize, with softmax\n",
    "# 5. The raw vectors may not be in the right latent space, let's add an extra linear transf.\n",
    "# 6. AND, let each 'raw vec term' in the weighted sum have its own customizable projection\n",
    "# 7. Voila! These transformed raw vectors are key, quary and values! \n",
    "#    And we get the output seq by good old weighted sum (weights as dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 5\n",
    "latent_dim = 7\n",
    "#init the matrices to get K, Q, V vecotors\n",
    "M_K, M_Q, M_V = [torch.rand(emb_dim, latent_dim) for _ in range(3)]\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3447, 0.8092, 0.3961, 0.4156, 0.9257, 0.4706, 0.7407],\n",
       "        [0.5121, 0.5361, 0.9129, 0.5644, 0.2540, 0.2610, 0.1731],\n",
       "        [0.6374, 0.6320, 0.2554, 0.5370, 0.6227, 0.6127, 0.1856],\n",
       "        [0.2040, 0.9485, 0.3082, 0.9250, 0.6534, 0.0748, 0.3550],\n",
       "        [0.2369, 0.1740, 0.7922, 0.8342, 0.0204, 0.1259, 0.6711]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.3665,  0.4639,  0.2141, -0.8021,  0.4635,  0.2811, -0.5971],\n",
       "         [ 0.1629, -0.5562,  1.3616,  0.8082, -0.5413,  0.1150,  1.2462],\n",
       "         [-0.2491, -1.2292,  1.0150, -0.5639, -0.7743, -0.0252,  1.0896],\n",
       "         [-0.4977, -1.0448, -1.5880, -2.4483,  0.0663,  0.2555, -0.4271],\n",
       "         [ 0.1031,  0.2549, -0.7498, -1.0659,  1.1129,  0.7434,  0.4696],\n",
       "         [ 0.9517,  1.6318,  0.8155,  0.7055,  1.8004,  1.1003,  1.0651],\n",
       "         [-0.7254, -0.4530, -1.5167, -1.0794, -0.2987, -0.5533, -1.0132],\n",
       "         [-1.2717, -2.1783, -1.5621, -1.9204, -2.0457, -1.2393, -1.7073]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x@M_K #this is how one transforms the whole batch of words!\n",
    "# the trick to applying matrix to a batch is really just put x first \n",
    "# it can be hard to make that mental switch from the math background\n",
    "# since in math we are so used to put the matrix before x, but nothing special is going on here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "K, Q, V = x@M_K, x@M_Q, x@M_V \n",
    "# to get all pairs of dot products between Q_i and K_j, we multiply the \"batch matrices\" like so\n",
    "# here matrices is truly a bookkeeping device\n",
    "# and the matrix multiplication here is just a neat notation to get all pairs\n",
    "# almost like list comprehension is a neat notation for for loop, the essence remains basic\n",
    "# W_ij = Q_i . K_j\n",
    "W_raw = Q@(K.transpose(1,2))\n",
    "# when we do the final sum, we are summing over (or contracting the index of;) j -- the keys\n",
    "# and we softmax these weights, i.e softmax all the rows of W\n",
    "W = F.softmax(W_raw, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check softmax does indeed makes probabilities\n",
    "torch.sum(W, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4231, -0.6297, -0.1127, -0.1433, -0.3924,  0.1836, -0.0788],\n",
       "         [ 0.2189, -0.1688,  0.0991,  0.1640, -0.1528,  0.0889, -0.0864],\n",
       "         [ 0.6795, -0.8617,  0.1346,  0.2751, -0.7951,  0.1563, -0.5765],\n",
       "         [ 0.4446, -0.6852, -0.0218, -0.0228, -0.5302,  0.0906, -0.3964],\n",
       "         [ 0.4071, -0.5548,  0.0055, -0.0293, -0.3627,  0.1148, -0.2745],\n",
       "         [ 2.1575, -0.5151,  1.7189,  1.3461,  0.4325,  1.2230,  0.4569],\n",
       "         [ 0.0795, -0.1965, -0.0770, -0.0895, -0.1347,  0.0139, -0.0796],\n",
       "         [-2.4305, -3.1821, -4.7890, -5.0264, -2.9587, -2.2738, -3.3613]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting it together: y_i = \\sum_j{ W_i_j * v_j}\n",
    "# in matrix form -- one could check by hand, it is\n",
    "Y = W@V\n",
    "# so yeah, batching can use matrix operations to bookkeep things and results elegantly!\n",
    "Y\n",
    "# num_words x latent dim -- shape checks out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiheaded attention (wide) is just repeating the process a few times to get different Y's\n",
    "# this way we can have multiple customizable \"contexts\" in these Y's\n",
    "# the hope is that backprop will tune it such that each Y represents a disentabgled context\n",
    "# Well then, let's make attention a function and call it repeatedly\n",
    "def self_attention(emb_dim, latent_dim):\n",
    "    M_K, M_Q, M_V = [torch.rand(emb_dim, latent_dim) for _ in range(3)]\n",
    "    K, Q, V = x@M_K, x@M_Q, x@M_V \n",
    "    W_raw = Q@(K.transpose(1,2))\n",
    "    W = F.softmax(W_raw, dim=1)\n",
    "    Y = W@V\n",
    "    return Y\n",
    "num_heads = 9\n",
    "Ys = [self_attention(5,7) for _ in range(num_heads)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[ 9.6245e-03,  3.8510e-03, -1.0166e-01, -7.6849e-02,  7.4552e-02,\n",
       "           -1.7081e-01, -4.0388e-02],\n",
       "          [-3.8288e-03, -1.7216e-02, -7.1027e-02, -8.6520e-02,  5.8888e-02,\n",
       "           -1.3025e-01, -3.6269e-02],\n",
       "          [ 1.9010e-02,  4.5856e-03, -1.0785e-01, -5.3068e-02,  1.0703e-01,\n",
       "           -1.9431e-01, -3.6337e-02],\n",
       "          [ 1.5591e-01,  9.3957e-02, -2.9435e-01,  5.6613e-03,  4.2887e-01,\n",
       "           -5.7024e-01, -1.8289e-02],\n",
       "          [ 6.0503e-02,  5.7887e-02, -2.6099e-02,  4.9209e-02,  6.8625e-02,\n",
       "           -4.8756e-02,  3.5670e-02],\n",
       "          [ 2.3451e+00,  2.0204e+00,  5.1652e-01,  1.3334e+00,  1.9145e+00,\n",
       "            5.1800e-01,  2.2547e+00],\n",
       "          [ 3.6452e-02,  1.1894e-02, -1.1123e-01, -4.5079e-02,  1.4220e-01,\n",
       "           -2.1602e-01, -2.5534e-02],\n",
       "          [-3.5185e+00, -3.9234e+00, -3.6839e+00, -4.0448e+00,  6.6813e-01,\n",
       "           -7.2185e+00, -5.1851e+00]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 0.2039,  0.0933,  0.1230,  0.0290, -0.1245, -0.2357,  0.2336],\n",
       "          [ 0.2724, -0.0625,  0.4405, -0.3169, -1.0688, -0.1443,  0.2983],\n",
       "          [ 0.0430, -0.0466,  0.1570, -0.0398, -0.4029, -0.0532,  0.1207],\n",
       "          [-1.5205, -1.7870, -1.1655, -1.3868, -1.3273, -0.8402, -0.9456],\n",
       "          [ 0.0444, -0.0234,  0.0461, -0.0528, -0.1699, -0.1149,  0.0896],\n",
       "          [ 2.5549,  2.0180,  1.9789,  0.7199, -0.2062,  0.0657,  1.8521],\n",
       "          [ 0.1939, -0.1706, -0.2766, -0.6442, -0.1578, -0.9185,  0.2771],\n",
       "          [-3.2535, -3.9257, -2.7360, -3.2146, -2.7049, -2.1829, -2.0174]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 0.1521,  0.0778, -0.0552, -0.0629,  0.0263,  0.0202,  0.0451],\n",
       "          [ 0.3710,  0.1407, -0.1827, -0.2148,  0.0212,  0.0259,  0.0393],\n",
       "          [-0.0476, -0.0743, -0.1108, -0.0942,  0.1138,  0.1176,  0.0649],\n",
       "          [-1.6528, -1.8566, -1.7079, -1.2386, -0.7712, -0.4413, -0.3509],\n",
       "          [ 0.0503,  0.0139, -0.0396, -0.0418,  0.0168,  0.0174,  0.0158],\n",
       "          [ 2.9641,  2.7680,  0.7774,  0.6849,  1.1167,  0.5947,  1.7851],\n",
       "          [ 0.1489,  0.0341, -0.1155, -0.1280,  0.0388,  0.0444,  0.0252],\n",
       "          [-2.1510, -2.7936, -3.2362, -2.4045, -0.9388, -0.4213, -0.1857]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[-0.3780, -0.3231,  0.0431, -0.3787, -0.0376, -0.0911, -0.2195],\n",
       "          [-0.0540, -0.3560,  0.1349,  0.0918,  0.6675,  0.2543,  0.0871],\n",
       "          [-0.3892, -0.3660,  0.0159, -0.3421, -0.0209, -0.0836, -0.2466],\n",
       "          [-0.1119, -0.1120, -0.0053, -0.0874, -0.0105, -0.0154, -0.0826],\n",
       "          [-0.3512, -0.5823, -0.1312, -0.0244,  0.1309,  0.1828, -0.4000],\n",
       "          [-0.0259, -1.6366, -0.2028,  1.3538,  1.7986,  1.7289, -0.4688],\n",
       "          [-0.2331, -0.2070, -0.0264, -0.1803, -0.0511, -0.1222, -0.1520],\n",
       "          [-5.0252, -3.2901, -2.2017, -3.5691, -4.9832, -4.2927, -4.5524]]],\n",
       "        grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 1.6448e-01,  5.4719e-02,  1.3248e-01, -9.9074e-02,  2.0132e-01,\n",
       "            2.8950e-01,  9.7447e-02],\n",
       "          [ 3.9491e-02,  4.8819e-02,  4.6804e-02, -3.9510e-03,  5.6591e-02,\n",
       "            4.6950e-02,  4.5450e-02],\n",
       "          [ 3.7879e-02,  3.3054e-02,  3.8276e-02, -1.1607e-02,  5.2876e-02,\n",
       "            5.3072e-02,  3.5821e-02],\n",
       "          [ 1.1871e-01,  1.7018e-02, -3.6130e-03, -7.2218e-02,  2.9929e-01,\n",
       "            1.3549e-01,  1.1379e-01],\n",
       "          [ 8.4750e-02,  2.1633e-02,  6.3537e-02, -5.5106e-02,  1.0647e-01,\n",
       "            1.5174e-01,  4.7927e-02],\n",
       "          [ 2.3469e+00,  2.6692e+00,  2.9062e+00,  3.0117e-01,  2.1203e+00,\n",
       "            2.7838e+00,  1.8945e+00],\n",
       "          [ 2.7658e-01,  7.2402e-02,  1.9618e-01, -2.5338e-01,  4.5248e-01,\n",
       "            5.3194e-01,  2.0757e-01],\n",
       "          [-5.4097e+00, -3.9684e+00, -5.4346e+00, -5.1272e+00,  1.0630e+00,\n",
       "           -3.6172e+00, -4.4157e-01]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 2.6578e-01, -1.0414e-01,  3.5241e-01,  3.7023e-01,  4.4999e-03,\n",
       "            4.7731e-03,  1.5947e-01],\n",
       "          [ 7.6599e-02, -3.5371e-02,  1.3667e-01,  1.1412e-01, -2.0285e-02,\n",
       "            2.7461e-02,  1.3302e-02],\n",
       "          [ 5.4783e-02, -1.7216e-01,  4.7080e-01,  1.5673e-01, -2.5782e-01,\n",
       "            2.2595e-01, -4.0686e-01],\n",
       "          [-9.7402e-01, -4.0308e-01, -9.7478e-03, -1.2026e+00, -1.0196e+00,\n",
       "            2.9525e-01, -2.4681e+00],\n",
       "          [-2.1155e-02, -7.2699e-03,  1.3125e-01, -8.0327e-02, -1.0147e-01,\n",
       "            1.0751e-01, -3.1989e-01],\n",
       "          [ 2.1320e+00,  7.7181e-01,  2.7566e+00,  2.0421e+00,  5.9480e-01,\n",
       "            1.4651e+00,  8.1733e-01],\n",
       "          [-5.2581e-02,  3.8422e-02,  9.6802e-02, -7.3045e-02, -9.6239e-02,\n",
       "            1.5235e-01, -2.2388e-01],\n",
       "          [-4.8263e+00, -2.0273e+00, -2.7430e+00, -4.4675e+00, -3.5135e+00,\n",
       "           -6.7216e-01, -6.0806e+00]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[ 2.5541e-03, -4.3063e-02, -2.0301e-01,  5.6741e-02,  1.0195e-01,\n",
       "           -1.4374e-01, -2.9083e-01],\n",
       "          [ 3.0070e-01, -5.3949e-02,  4.0601e-02,  1.4702e-02,  1.1655e-01,\n",
       "            3.4967e-01,  1.6982e-01],\n",
       "          [ 6.7160e-02, -3.8315e-02, -4.0629e-03, -8.9265e-03,  2.2088e-02,\n",
       "            8.0308e-02,  1.3484e-02],\n",
       "          [-1.2821e-01, -3.2671e-01, -1.9200e-01, -1.7562e-01,  1.5060e-02,\n",
       "            4.3145e-01, -8.9587e-02],\n",
       "          [ 2.4400e-01, -1.3256e-01,  7.6240e-02, -6.7265e-02,  3.1092e-02,\n",
       "            3.7455e-01,  1.8294e-01],\n",
       "          [ 1.0585e+00,  7.3068e-01, -2.9830e-01,  8.5153e-01,  1.0850e+00,\n",
       "            4.5539e-01,  2.2933e-01],\n",
       "          [ 8.1139e-02, -2.3534e-01, -6.2710e-02, -1.2952e-01,  2.0332e-02,\n",
       "            3.9477e-01,  3.1590e-02],\n",
       "          [-6.6169e+00, -2.6025e+00, -2.9093e+00, -1.5499e+00, -1.4854e+00,\n",
       "           -2.1360e+00, -4.4313e+00]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[-4.4604e-03,  1.5933e-02,  2.6293e-03,  5.8153e-03,  1.9052e-03,\n",
       "            2.6009e-02,  2.8702e-02],\n",
       "          [ 1.0468e-01,  1.6324e-01,  6.8580e-02, -3.5327e-02, -2.4234e-01,\n",
       "            5.5568e-02,  1.3577e-01],\n",
       "          [-2.0000e-03,  7.3520e-04, -4.3105e-03, -4.3621e-03, -3.8226e-03,\n",
       "            1.0698e-04,  2.4789e-03],\n",
       "          [-4.3044e-02, -1.8658e-02, -8.5360e-02, -8.4848e-02, -5.8152e-02,\n",
       "           -2.7125e-02,  3.6779e-03],\n",
       "          [ 1.7089e-03,  1.1289e-02, -3.4718e-03, -6.9849e-03, -1.5376e-02,\n",
       "            6.1949e-03,  1.4347e-02],\n",
       "          [ 2.0326e-01,  2.0550e+00,  2.5829e+00,  2.6196e+00,  1.2262e+00,\n",
       "            3.8833e+00,  3.1339e+00],\n",
       "          [-4.6410e-02,  1.1543e-02, -1.0742e-01, -1.2954e-01, -1.3646e-01,\n",
       "           -1.7186e-02,  4.4228e-02],\n",
       "          [-1.9787e+00, -2.5251e+00, -4.7504e+00, -4.9533e+00, -3.1872e+00,\n",
       "           -3.4569e+00, -2.3012e+00]]], grad_fn=<UnsafeViewBackward>),\n",
       " tensor([[[-6.0970e-02,  8.2504e-03, -1.6321e-02,  5.4571e-02,  1.5305e-02,\n",
       "            3.7341e-02,  5.5404e-02],\n",
       "          [ 3.1674e-01,  1.1331e+00,  6.8158e-01,  1.1873e+00,  1.0036e+00,\n",
       "            1.1073e+00,  1.2149e+00],\n",
       "          [ 5.7280e-02,  1.9124e-01,  2.8396e-02,  2.8605e-01,  1.1751e-01,\n",
       "            9.3024e-02,  4.6687e-02],\n",
       "          [-1.2266e-02,  3.9834e-01, -2.1656e-03,  6.7367e-01,  2.0178e-01,\n",
       "            2.2362e-01,  7.8415e-02],\n",
       "          [-4.6256e-02,  7.4876e-02, -1.9042e-03,  1.4272e-01,  4.5607e-02,\n",
       "            7.4074e-02,  6.1475e-02],\n",
       "          [-2.5027e-01,  4.5668e-01,  5.0719e-01,  4.5817e-01,  5.2574e-01,\n",
       "            8.8438e-01,  1.2341e+00],\n",
       "          [-4.5540e-01, -1.1429e-02, -1.5367e-01,  3.1584e-01,  7.0403e-02,\n",
       "            2.1340e-01,  3.6103e-01],\n",
       "          [-3.4615e+00, -2.7839e+00, -2.7803e+00, -1.2372e+00, -3.0040e+00,\n",
       "           -1.9216e+00, -2.6523e+00]]], grad_fn=<UnsafeViewBackward>)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
