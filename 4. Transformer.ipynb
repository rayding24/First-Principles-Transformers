{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "peripheral-amsterdam",
   "metadata": {},
   "source": [
    "Having gone through all the key components, we can construct a GPT style autoregressive Transformer model by simply stacking blocks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "  class Block(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        self.ln1 = nn.LayerNorm(emb_dim)\n",
    "        self.ln2 = nn.LayerNorm(emb_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 2*emb_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2*emb_dim, emb_dim) ) # can be any mlp, this is one simple example\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "\n",
    "    def self_attention(self, x, emb_dim,):\n",
    "        M_K, M_Q, M_V = [nn.Linear(emb_dim, emb_dim, bias=False) for _ in range(3)]\n",
    "        K, Q, V = [M(x) for M in [M_K, M_Q, M_V ]]\n",
    "        W_raw = Q@(K.transpose(-1,-2))\n",
    "        # == masking begins ==\n",
    "        ones = torch.ones((seq_len, seq_len), dtype=torch.uint8)\n",
    "        mask = torch.triu(ones, diagonal=1)\n",
    "        W_raw[mask] = float('-inf')\n",
    "        # == masking ends ==\n",
    "        W = F.softmax(W_raw, dim=-1)\n",
    "        Y = W@V\n",
    "        return Y\n",
    "\n",
    "\n",
    "\n",
    "    def forward(x):\n",
    "        x = x + self.self_attention(x, self.emb_dim)\n",
    "        x_ln1 = self.ln1(x)\n",
    "        x_mlp = self.mlp(x)\n",
    "        x = x_ln1 + x_mlp\n",
    "        x = ln2(x)\n",
    "        return x \n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, emb_dim, vocab_size, num_blocks, num_classes):\n",
    "        self.blocks = nn.Sequential(*[Block(emb_dim) for _ in num_blocks])\n",
    "        self.word_embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def positional_encoding(self, max_len, d_model):\n",
    "    '''\n",
    "    Computes positional embedding vectors deterministically with sin and cos\n",
    "    max_len: number of positions, i.e. input seq. length\n",
    "    d_model: embedding dimensiion\n",
    "    \n",
    "    CAVEAT/WARNING: the embedding dimension must be even, as dictated by the formula\n",
    "    '''\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    #     pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        return pe\n",
    "\n",
    "    \n",
    "    def forward(self):\n",
    "        seq_len = len(x[0]) #assume first dimension is for batching\n",
    "        pe = self.positional_encoding(seq_len, self.emb_dim)\n",
    "        x = self.word_embedding(x) + pe \n",
    "        x = self.blocks(x) \n",
    "        x = self.head(x)\n",
    "        x = F.softmax(x, dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "german-election",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "emb_dim = 4\n",
    "seq_len = 5\n",
    "\n",
    "#input tokens:\n",
    "x = torch.tensor([0,1,2,3,4]) \n",
    "embedding = nn.Embedding(20, emb_dim) #vocab size 20, emb dim 4\n",
    "x = embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "naval-arctic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5961,  1.6146, -0.5593,  1.3978],\n",
       "        [ 0.0600, -1.4765,  0.3478, -0.6228],\n",
       "        [-0.2879,  1.2764,  0.3876, -0.1122],\n",
       "        [-1.6582,  0.9503, -1.5498, -1.7656],\n",
       "        [ 0.5006,  0.4357, -1.1175, -1.0587]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "handled-adoption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.5963,  0.5601, -0.4982, -0.4323], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "authorized-sociology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.2142, -0.4229,  0.3160, -1.0058, -0.3100], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "standing-guidance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8832, -0.6725, -2.8464, -0.8893],\n",
       "        [-1.1164, -2.6529, -0.8286, -1.7992],\n",
       "        [-2.1902, -0.6258, -1.5147, -2.0144],\n",
       "        [-2.8089, -0.2004, -2.7004, -2.9163],\n",
       "        [-0.8526, -0.9175, -2.4707, -2.4119]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specified-break",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0206, 0.5104, 0.0581, 0.4109],\n",
       "        [0.3274, 0.0704, 0.4367, 0.1654],\n",
       "        [0.1119, 0.5348, 0.2199, 0.1334],\n",
       "        [0.0603, 0.8184, 0.0672, 0.0541],\n",
       "        [0.4263, 0.3995, 0.0845, 0.0896]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = F.softmax(x, dim=-1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "endless-longer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9465, 2.3336, 0.8663, 0.8536], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(y, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-pixel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
